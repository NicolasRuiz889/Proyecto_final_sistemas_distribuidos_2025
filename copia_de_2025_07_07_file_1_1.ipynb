{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NicolasRuiz889/Proyecto_final_sistemas_distribuidos_2025/blob/main/copia_de_2025_07_07_file_1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ac1816"
      },
      "source": [
        "\\# MODELO DE PREDICCI√ìN PARA OFFERED CALLS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "d6abab"
      },
      "source": [
        "\\#\\#\\# 1. Importaci√≥n de librerias\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install skforecast"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pqAEm9taw-q",
        "outputId": "61192532-3c51-43db-8385-40cee216d7ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: skforecast in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from skforecast) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.11/dist-packages (from skforecast) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.57 in /usr/local/lib/python3.11/dist-packages (from skforecast) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=1.2 in /usr/local/lib/python3.11/dist-packages (from skforecast) (1.6.1)\n",
            "Requirement already satisfied: optuna>=2.10 in /usr/local/lib/python3.11/dist-packages (from skforecast) (4.4.0)\n",
            "Requirement already satisfied: joblib>=1.1 in /usr/local/lib/python3.11/dist-packages (from skforecast) (1.5.1)\n",
            "Requirement already satisfied: numba>=0.59 in /usr/local/lib/python3.11/dist-packages (from skforecast) (0.60.0)\n",
            "Requirement already satisfied: rich>=13.9 in /usr/local/lib/python3.11/dist-packages (from skforecast) (13.9.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.59->skforecast) (0.43.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna>=2.10->skforecast) (1.16.4)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna>=2.10->skforecast) (6.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna>=2.10->skforecast) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna>=2.10->skforecast) (2.0.41)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna>=2.10->skforecast) (6.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->skforecast) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->skforecast) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->skforecast) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.9->skforecast) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.9->skforecast) (2.19.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2->skforecast) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2->skforecast) (3.6.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna>=2.10->skforecast) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna>=2.10->skforecast) (4.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.9->skforecast) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5->skforecast) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna>=2.10->skforecast) (3.2.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install feature_engine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iu6kGEuLbOpt",
        "outputId": "2cea65b4-e8cb-4fdb-8644-a47e5bd130e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting feature_engine\n",
            "  Downloading feature_engine-1.8.3-py2.py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.11/dist-packages (from feature_engine) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from feature_engine) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from feature_engine) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from feature_engine) (1.15.3)\n",
            "Requirement already satisfied: statsmodels>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from feature_engine) (0.14.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.0->feature_engine) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.0->feature_engine) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.0->feature_engine) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.0->feature_engine) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.0->feature_engine) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.11.1->feature_engine) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.11.1->feature_engine) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->feature_engine) (1.17.0)\n",
            "Downloading feature_engine-1.8.3-py2.py3-none-any.whl (378 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m378.6/378.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: feature_engine\n",
            "Successfully installed feature_engine-1.8.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install astral"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWu4fYxPbZ4w",
        "outputId": "f199d097-298e-49da-e4fc-3ce97cd00ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting astral\n",
            "  Downloading astral-3.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading astral-3.2-py3-none-any.whl (38 kB)\n",
            "Installing collected packages: astral\n",
            "Successfully installed astral-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3e707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "06739b55-a7b7-464d-f930-409e4e49a87b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[38;5;208mVersion skforecast: 0.16.0\n",
            "\u001b[1m\u001b[38;5;208mVersion scikit-learn: 1.6.1\n",
            "\u001b[1m\u001b[38;5;208mVersion lightgbm: 4.5.0\n",
            "\u001b[1m\u001b[38;5;208mVersion pandas: 2.2.2\n",
            "\u001b[1m\u001b[38;5;208mVersion numpy: 2.0.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap>:1047: ImportWarning:\n",
            "\n",
            "_PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning:\n",
            "\n",
            "_BokehImportHook.find_spec() not found; falling back to find_module()\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#==============================================================================\n",
        "# ‚öôÔ∏è Configuraci√≥n inicial\n",
        "# ==============================================================================\n",
        "import warnings\n",
        "warnings.filterwarnings('once')\n",
        "\n",
        "# Colores para impresi√≥n\n",
        "color = '\\033[1m\\033[38;5;208m'\n",
        "\n",
        "# ==============================================================================\n",
        "# üßÆ Librer√≠as base\n",
        "# ==============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from io import StringIO\n",
        "import contextlib\n",
        "\n",
        "# ==============================================================================\n",
        "# üìä Visualizaci√≥n\n",
        "# ==============================================================================\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "plt.rcParams.update({'font.size': 8})\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "import plotly.offline as poff\n",
        "pio.templates.default = \"seaborn\"\n",
        "poff.init_notebook_mode(connected=True)\n",
        "\n",
        "# Statsmodels\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.stattools import acf, pacf\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# skforecast visual tools\n",
        "from skforecast.plot import (\n",
        "    plot_residuals,\n",
        "    calculate_lag_autocorrelation,\n",
        "    set_dark_theme\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# üì¶ Librer√≠as de modelos y forecasting\n",
        "# ==============================================================================\n",
        "import sklearn\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.feature_selection import RFECV\n",
        "\n",
        "import lightgbm\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "import shap\n",
        "\n",
        "# skforecast (forecasting con regresores)\n",
        "import skforecast\n",
        "from skforecast.datasets import fetch_dataset\n",
        "from skforecast.feature_selection import select_features\n",
        "from skforecast.preprocessing import RollingFeatures\n",
        "from skforecast.model_selection import (\n",
        "    TimeSeriesFold,\n",
        "    backtesting_forecaster,\n",
        "    backtesting_sarimax,\n",
        "    bayesian_search_forecaster,\n",
        "    grid_search_sarimax\n",
        ")\n",
        "from skforecast.direct import ForecasterDirect\n",
        "from skforecast.recursive import (\n",
        "    ForecasterRecursive,\n",
        "    ForecasterEquivalentDate,\n",
        "    ForecasterSarimax\n",
        ")\n",
        "from skforecast.metrics import calculate_coverage\n",
        "from skforecast.sarimax import Sarimax\n",
        "\n",
        "# ==============================================================================\n",
        "# üïí Ingenier√≠a de caracter√≠sticas temporales\n",
        "# ==============================================================================\n",
        "from feature_engine.datetime import DatetimeFeatures\n",
        "from feature_engine.creation import CyclicalFeatures\n",
        "from feature_engine.timeseries.forecasting import WindowFeatures\n",
        "\n",
        "# Astral para variables solares\n",
        "from astral import LocationInfo\n",
        "from astral.sun import sun\n",
        "\n",
        "# ==============================================================================\n",
        "# üîé Mostrar versiones principales\n",
        "# ==============================================================================\n",
        "print(f\"{color}Version skforecast: {skforecast.__version__}\")\n",
        "print(f\"{color}Version scikit-learn: {sklearn.__version__}\")\n",
        "print(f\"{color}Version lightgbm: {lightgbm.__version__}\")\n",
        "print(f\"{color}Version pandas: {pd.__version__}\")\n",
        "print(f\"{color}Version numpy: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amrrXT-NSYdH"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Leer todo el Excel\n",
        "file_path = r'C:\\Users\\nicol\\OneDrive\\Escritorio\\forecast\\data (20).xlsx'\n",
        "df_excel = pd.read_excel(file_path)\n",
        "\n",
        "# 2) Guardar la primera aparici√≥n de ‚ÄúTotal‚Äù en DAY - Year\n",
        "year_total_df = df_excel[df_excel['DAY - Year'] == 'Total'].head(1).copy()\n",
        "\n",
        "# 3) Encontrar el √≠ndice de esa fila\n",
        "mask_year_total   = df_excel['DAY - Year'] == 'Total'\n",
        "first_total_idx   = mask_year_total[mask_year_total].index[0]\n",
        "\n",
        "# 4) Cortar todo lo que viene desde esa fila en adelante\n",
        "df_trim = df_excel.iloc[:first_total_idx].copy()\n",
        "\n",
        "# 5) A partir de aqu√≠ trabajas solo con df_trim\n",
        "#    Por ejemplo, eliminar cualquier ‚ÄúTotal‚Äù residual en Month/Day/Hour\n",
        "df_clean = df_trim[\n",
        "    (df_trim['DAY - Month'] != 'Total') &\n",
        "    (df_trim['DAY - Day']   != 'Total') &\n",
        "    (df_trim['Hour']        != 'Total')\n",
        "].copy()\n",
        "\n",
        "# 6) (Opcional) ya no haces dropna si no quieres eliminar NaNs leg√≠timos\n",
        "# 7) Normalizar Hour, construir fecha, etc.\n",
        "\n",
        "# --- Al final tienes ---\n",
        "print(\"Fila de Total en Year guardada:\")\n",
        "print(year_total_df)\n",
        "print(\"\\nData limpia lista para procesar:\")\n",
        "print(df_clean.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oP4al9toSYdH"
      },
      "outputs": [],
      "source": [
        "df_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2omwZ-J8SYdI"
      },
      "outputs": [],
      "source": [
        "# 2) Conteo de nulos por columna\n",
        "null_counts = df_clean.isna().sum().reset_index()\n",
        "null_counts.columns = ['Column', 'Null Count']\n",
        "print(\"Null counts per column:\")\n",
        "print(null_counts.to_string(index=False))\n",
        "\n",
        "# 3) Filas con al menos un nulo\n",
        "nan_rows = df_clean[df_clean.isna().any(axis=1)]\n",
        "print(\"\\nRows containing any nulls:\")\n",
        "print(nan_rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFyjcw7OSYdI"
      },
      "outputs": [],
      "source": [
        "# 1) Normaliza Hour para quedarte solo con \"HH:MM\"\n",
        "df_clean['Hour_str'] = df_clean['Hour'].astype(str).str[:5]\n",
        "\n",
        "# 2) Construye la cadena y parsea\n",
        "fecha_str = (\n",
        "    df_clean['DAY - Year'].astype(int).astype(str)   + '-' +\n",
        "    df_clean['DAY - Month']                          + '-' +\n",
        "    df_clean['DAY - Day'].astype(int).astype(str)    + ' ' +\n",
        "    df_clean['Hour_str']\n",
        ")\n",
        "df_clean['fecha'] = pd.to_datetime(fecha_str,\n",
        "                                   format='%Y-%B-%d %H:%M',\n",
        "                                   errors='raise')  # te avisar√° si a√∫n hay algo mal\n",
        "\n",
        "# 3) (Opcional) quita la columna auxiliar\n",
        "df_clean.drop(columns=['Hour_str'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sD3q1hEoSYdI"
      },
      "outputs": [],
      "source": [
        "df_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkwCrudMSYdJ"
      },
      "outputs": [],
      "source": [
        "# 7. Guardar el resultado limpio\n",
        "output_csv = r'C:\\Users\\nicol\\OneDrive\\Escritorio\\forecast\\output.csv'\n",
        "df_clean.to_csv(output_csv, index=False)\n",
        "\n",
        "print(\"CSV limpio guardado en:\", output_csv)\n",
        "print(\"\\nFila de total mensual extra√≠da:\")\n",
        "print(month_total_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3_QvcJQSYdJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --- 1. Carga y preparaci√≥n de df1 ---\n",
        "df1 = pd.read_csv('output.csv', parse_dates=['fecha'], index_col='fecha')\n",
        "\n",
        "df1 = df1.rename(columns={\n",
        "    'Offered':   'OFF Calls',\n",
        "    'Handled':   'Handled Calls',\n",
        "    'AHT':       'Real AHT',\n",
        "    'TSF':       'Real TSF',\n",
        "    'ABA %':     'Real ABA%',\n",
        "    'Calls SL':  'Real ABA',\n",
        "    'ASA':       'Real ASA'\n",
        "})\n",
        "\n",
        "# df1 ya estaba en float, pero nos aseguramos de que TSF y ABA% sean float\n",
        "df1['Real TSF']  = df1['Real TSF'].astype(float)\n",
        "df1['Real ABA%'] = df1['Real ABA%'].astype(float)\n",
        "\n",
        "cols = ['OFF Calls','Handled Calls','Real AHT','Real TSF','Real ABA%','Real ASA','Real ABA']\n",
        "df1 = df1[cols]\n",
        "\n",
        "# --- 2. Carga y preparaci√≥n de df2 ---\n",
        "df2 = pd.read_csv('data_jam.csv')\n",
        "\n",
        "meses = {\n",
        "    'enero':'January','febrero':'February','marzo':'March','abril':'April',\n",
        "    'mayo':'May','junio':'June','julio':'July','agosto':'August',\n",
        "    'septiembre':'September','octubre':'October','noviembre':'November','diciembre':'December'\n",
        "}\n",
        "\n",
        "df2['mes_ing'] = df2['DATE_S - Mes'].str.lower().map(meses)\n",
        "df2['fecha']   = pd.to_datetime(\n",
        "    df2['DATE_S - D√≠a'].astype(str) + ' ' +\n",
        "    df2['mes_ing'] + ' ' +\n",
        "    df2['DATE_S - A√±o'].astype(str),\n",
        "    format='%d %B %Y',\n",
        "    errors='coerce'\n",
        ")\n",
        "\n",
        "# Generar √≠ndice continuo cada 30 minutos\n",
        "inicio = df2['fecha'].iloc[0]\n",
        "df2['fecha'] = [inicio + pd.Timedelta(minutes=30 * i) for i in range(len(df2))]\n",
        "# 4. CONFIGURACI√ìN DE SERIE TEMPORAL\n",
        "df2 = df2.set_index('fecha')\n",
        "df2 = df2.sort_index()\n",
        "df2 = df2.asfreq('30min')\n",
        "\n",
        "# Convertir TSF y ABA% a float\n",
        "df2['Real TSF']  = df2['Real TSF'].str.replace('%','').astype(float) / 100\n",
        "df2['Real ABA%'] = df2['Real ABA%'].str.replace('%','').astype(float) / 100\n",
        "\n",
        "\n",
        "# Asegurar el resto de columnas est√°n num√©ricas inicialmente\n",
        "df2['OFF Calls']     = df2['OFF Calls'].astype(int)\n",
        "df2['Handled Calls'] = df2['Handled Calls'].astype(int)\n",
        "df2['Real AHT']      = df2['Real AHT'].astype(int)\n",
        "df2['Real ASA']      = df2['Real ASA'].astype(int)\n",
        "df2['Real ABA']      = df2['Real ABA'].astype(int)\n",
        "\n",
        "df2 = df2[cols]\n",
        "\n",
        "# --- 3. Unificaci√≥n ---\n",
        "cutoff   = df2.index.max() + pd.Timedelta(minutes=30)\n",
        "df1_tail = df1[df1.index >= cutoff]\n",
        "df_final = pd.concat([df2, df1_tail])\n",
        "\n",
        "# --- 4. Downcast de enteros para ahorrar espacio ---\n",
        "int_cols = ['OFF Calls','Handled Calls','Real AHT','Real ASA','Real ABA']\n",
        "df_final[int_cols] = df_final[int_cols].apply(pd.to_numeric, downcast='integer')\n",
        "df_final[['Real TSF','Real ABA%']] = (df_final[['Real TSF','Real ABA%']] * 100).round(1)\n",
        "\n",
        "# --- 5. Guardar y mostrar resultados ---\n",
        "merged_path = 'merged_output.csv'\n",
        "df_final.to_csv(merged_path)\n",
        "print(\"Archivo unificado guardado como:\", merged_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4qVFSwoSYdK"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('merged_output.csv' , parse_dates=['fecha'], index_col='fecha')\n",
        "df.index = pd.to_datetime(df.index)\n",
        "df = df.asfreq('30min')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccf3d8"
      },
      "outputs": [],
      "source": [
        "df.head(80000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeEq3W7ASYdK"
      },
      "outputs": [],
      "source": [
        "df.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh7_sDjGSYdK"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-dmR0UYSYdL"
      },
      "outputs": [],
      "source": [
        "# 2) Conteo de nulos por columna\n",
        "null_counts = df.isna().sum().reset_index()\n",
        "null_counts.columns = ['Column', 'Null Count']\n",
        "print(\"Null counts per column:\")\n",
        "print(null_counts.to_string(index=False))\n",
        "\n",
        "# 3) Filas con al menos un nulo\n",
        "nan_rows = df[df.isna().any(axis=1)]\n",
        "print(\"\\nRows containing any nulls:\")\n",
        "print(nan_rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGQkQUy-SYdL"
      },
      "outputs": [],
      "source": [
        "for col in ['TSF','ABA %','AHT']:\n",
        "    df_clean[col] = df_clean[col].interpolate(\n",
        "        method='time',       # usa la distancia en el √≠ndice datetime\n",
        "        limit_direction='both'\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV4t-tMzSYdL"
      },
      "outputs": [],
      "source": [
        "# 2. Filtro a partir del 1-Oct-2024\n",
        "df_filt = df[df.index >= '2024-10-01']\n",
        "\n",
        "# 3. Redondeo TSF y ABA% a 1 decimal\n",
        "df_filt['Real TSF']  = df_filt['Real TSF'].round(1)\n",
        "df_filt['Real ABA%'] = df_filt['Real ABA%'].round(1)\n",
        "\n",
        "# 4. Agrupo y cuento cada par\n",
        "pair_counts = (\n",
        "    df_filt\n",
        "    .groupby(['Real TSF', 'Real ABA%'])\n",
        "    .size()\n",
        "    .reset_index(name='Count')\n",
        "    # Ordeno por Real TSF ascendente (y, de paso, por Real ABA% ascendente)\n",
        "    .sort_values(['Real TSF'])\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# 5. Muestro el resultado\n",
        "print(pair_counts.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15e536"
      },
      "outputs": [],
      "source": [
        "# Verify that a temporary index is complete\n",
        "# ==============================================================================\n",
        "start_date = df.index.min()\n",
        "end_date = df.index.max()\n",
        "complete_date_range = pd.date_range(start=start_date, end=end_date, freq=df.index.freq)\n",
        "is_index_complete = (df.index == complete_date_range).all()\n",
        "print(f\"Index complete: {is_index_complete}\")\n",
        "print(f\"Number of rows with missing values: {df.isnull().any(axis=1).mean()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peIRA5OeSYdM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# matriz booleana de NaNs\n",
        "mask = df.isna().values\n",
        "rows, cols = np.where(mask)\n",
        "\n",
        "for r, c in zip(rows, cols):\n",
        "    fecha = df.index[r]\n",
        "    columna = df.columns[c]\n",
        "    print(f\"NaN en fila √≠ndice={r} (fecha={fecha}), columna='{columna}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "989302"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "728d52"
      },
      "outputs": [],
      "source": [
        "print(\"Puntos faltantes:\", df.isna().sum().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d72017"
      },
      "outputs": [],
      "source": [
        "df.head(80000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "694855"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd9379"
      },
      "outputs": [],
      "source": [
        "df = df.loc['2023-01-01 00:00:00':'2025-06-30 23:59:00'].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HILGdQvSYdN"
      },
      "outputs": [],
      "source": [
        "df.index = pd.to_datetime(df.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d89089"
      },
      "outputs": [],
      "source": [
        "df.head(80000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bef011"
      },
      "outputs": [],
      "source": [
        "# ==== Divisi√≥n de serie temporal sin solapamientos ====\n",
        "def dividir_serie_temporal(df, col, end_train, end_validation):\n",
        "    \"\"\"\n",
        "    Divide una columna de un DataFrame indexado por fecha en tres subconjuntos:\n",
        "    entrenamiento (hasta end_train), validaci√≥n (despu√©s de end_train hasta end_validation),\n",
        "    y prueba (despu√©s de end_validation), sin solapamientos.\n",
        "    \"\"\"\n",
        "    serie = df[col]\n",
        "    # Asegurar tipos datetime\n",
        "    end_train = pd.to_datetime(end_train)\n",
        "    end_validation = pd.to_datetime(end_validation)\n",
        "    # determinar frecuencia\n",
        "    freq = df.index.freq or pd.infer_freq(df.index)\n",
        "    # calcular pasos siguientes\n",
        "    next_train = end_train + freq\n",
        "    next_val = end_validation + freq\n",
        "\n",
        "    data_train = serie.loc[:end_train].copy()\n",
        "    data_val = serie.loc[next_train:end_validation].copy()\n",
        "    data_test = serie.loc[next_val:].copy()\n",
        "\n",
        "    print(f\"üìä {col} - Train      : {data_train.index.min()} ‚Äî {data_train.index.max()}  (n={len(data_train)})\")\n",
        "    print(f\"üìä {col} - Validation : {data_val.index.min()} ‚Äî {data_val.index.max()}  (n={len(data_val)})\")\n",
        "    print(f\"üìä {col} - Test       : {data_test.index.min()} ‚Äî {data_test.index.max()}  (n={len(data_test)})\")\n",
        "\n",
        "    return data_train, data_val, data_test\n",
        "\n",
        "\n",
        "# ==== Gr√°fica de particiones con l√≠neas verticales y retorno de figura ====\n",
        "def graficar_particiones_temporales(data_train, data_val, data_test,\n",
        "                                    variable, end_train, end_validation):\n",
        "    \"\"\"\n",
        "    Grafica las particiones train/validation/test con l√≠neas que marcan las fronteras.\n",
        "    Devuelve el objeto fig para guardado o modificaci√≥n posterior.\n",
        "    \"\"\"\n",
        "    set_dark_theme()\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=data_train.index, y=data_train,\n",
        "                             mode='lines', name='Train'))\n",
        "    fig.add_trace(go.Scatter(x=data_val.index, y=data_val,\n",
        "                             mode='lines', name='Validation'))\n",
        "    fig.add_trace(go.Scatter(x=data_test.index, y=data_test,\n",
        "                             mode='lines', name='Test'))\n",
        "    # L√≠neas de partici√≥n\n",
        "    fig.add_vline(x=pd.to_datetime(end_train), line=dict(color='gray', dash='dash'))\n",
        "    fig.add_vline(x=pd.to_datetime(end_validation), line=dict(color='gray', dash='dash'))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=f'Intraday 30-minutes {variable}',\n",
        "        xaxis_title=\"Time\",\n",
        "        yaxis_title=variable,\n",
        "        legend_title=\"Partition:\",\n",
        "        width=800, height=400,\n",
        "        margin=dict(l=20, r=20, t=35, b=20),\n",
        "        legend=dict(orientation=\"h\", yanchor=\"top\", y=1, xanchor=\"left\", x=0.001)\n",
        "    )\n",
        "    fig.show()\n",
        "    return fig\n",
        "\n",
        "\n",
        "# ==== Prueba de estacionariedad con interpretaci√≥n de p-value ====\n",
        "def probar_estacionariedad(df, columna, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Aplica pruebas ADF y KPSS a la serie y sus primeras dos diferencias,\n",
        "    e interpreta si cada serie es estacionaria seg√∫n alpha.\n",
        "    \"\"\"\n",
        "    serie = df[columna].dropna()\n",
        "    data_diff_1 = serie.diff().dropna()\n",
        "    data_diff_2 = data_diff_1.diff().dropna()\n",
        "\n",
        "    for data, orden in zip([serie, data_diff_1, data_diff_2], [0,1,2]):\n",
        "        adf = adfuller(data)\n",
        "        kpss_ = kpss(data, nlags=\"auto\")\n",
        "        if orden == 0:\n",
        "            print(f\"\\nüìà Serie original:\")\n",
        "        else:\n",
        "            print(f\"\\nüîÅ Diferenciada orden {orden}:\")\n",
        "        print(f\"ADF  ‚Üí estad√≠stico: {adf[0]:.4f}, p-valor: {adf[1]:.4f} \"\n",
        "              f\"-> {'Estacionaria' if adf[1]<alpha else 'No estacionaria'}\")\n",
        "        print(f\"KPSS ‚Üí estad√≠stico: {kpss_[0]:.4f}, p-valor: {kpss_[1]:.4f} \"\n",
        "              f\"-> {'No estacionaria' if kpss_[1]<alpha else 'Estacionaria'}\")\n",
        "\n",
        "    # Gr√°ficas\n",
        "    fig, axs = plt.subplots(nrows=3, ncols=1, figsize=(7,5), sharex=True)\n",
        "    serie.plot(ax=axs[0], title='Serie original')\n",
        "    data_diff_1.plot(ax=axs[1], title='Diferenciada orden 1')\n",
        "    data_diff_2.plot(ax=axs[2], title='Diferenciada orden 2')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# ==== Gr√°fica completa y zoom robusto ====\n",
        "def graficar_con_zoom(df, columna, zoom_range):\n",
        "    \"\"\"\n",
        "    Grafica la serie completa y un zoom en zoom_range usando m√°scara para fill_between.\n",
        "    \"\"\"\n",
        "    serie = df[columna]\n",
        "    inicio, fin = pd.to_datetime(zoom_range[0]), pd.to_datetime(zoom_range[1])\n",
        "\n",
        "    fig = plt.figure(figsize=(8,4))\n",
        "    grid = GridSpec(nrows=8, ncols=1, hspace=0.6, wspace=0)\n",
        "\n",
        "    main_ax = fig.add_subplot(grid[:3,:])\n",
        "    serie.plot(ax=main_ax, linewidth=0.5, alpha=0.5)\n",
        "    mask = (serie.index >= inicio) & (serie.index <= fin)\n",
        "    main_ax.fill_between(serie.index[mask], serie.min(), serie.max(),\n",
        "                         facecolor='blue', alpha=0.5, zorder=0)\n",
        "    main_ax.set_title(\n",
        "        f\"{columna} - Serie completa: {serie.index.min().date()} a {serie.index.max().date()}\")\n",
        "\n",
        "    zoom_ax = fig.add_subplot(grid[5:,:])\n",
        "    serie.loc[inicio:fin].plot(ax=zoom_ax, linewidth=1)\n",
        "    zoom_ax.set_title(f\"{columna} - Zoom: {inicio} a {fin}\")\n",
        "\n",
        "    plt.subplots_adjust(hspace=1)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ==== Boxplots estacionales con etiquetas legibles ====\n",
        "def graficar_boxplots_estacionalidad(df, columna):\n",
        "    \"\"\"\n",
        "    Boxplots por mes, d√≠a de la semana (con etiquetas) y hora, m√°s promedio por d√≠a-hora.\n",
        "    \"\"\"\n",
        "    df_aux = df[[columna]].dropna().copy()\n",
        "    df_aux['month'] = df_aux.index.month\n",
        "    df_aux['week_day'] = df_aux.index.dayofweek  # 0=Lun\n",
        "    df_aux['hour_day'] = df_aux.index.hour\n",
        "\n",
        "    # mensual\n",
        "    fig, ax = plt.subplots(figsize=(7,3))\n",
        "    df_aux.boxplot(column=columna, by='month', ax=ax,\n",
        "                   flierprops={'markersize':3, 'alpha':0.3})\n",
        "    df_aux.groupby('month')[columna].median().plot(style='o-', linewidth=0.8, ax=ax)\n",
        "    ax.set_title(f'{columna} - Distribuci√≥n mensual')\n",
        "    ax.set_xlabel('Mes')\n",
        "    plt.suptitle('')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # semanal\n",
        "    fig, ax = plt.subplots(figsize=(7,3))\n",
        "    df_aux.boxplot(column=columna, by='week_day', ax=ax,\n",
        "                   flierprops={'markersize':3, 'alpha':0.3})\n",
        "    day_labels = [\"Lun\",\"Mar\",\"Mi√©\",\"Jue\",\"Vie\",\"S√°b\",\"Dom\"]\n",
        "    ax.set_xticklabels(day_labels)\n",
        "    ax.set_title(f'{columna} - Distribuci√≥n semanal')\n",
        "    ax.set_ylabel(columna)\n",
        "    plt.suptitle('')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # horaria\n",
        "    fig, ax = plt.subplots(figsize=(7,3))\n",
        "    df_aux.boxplot(column=columna, by='hour_day', ax=ax,\n",
        "                   flierprops={'markersize':3, 'alpha':0.3})\n",
        "    ax.set_title(f'{columna} - Distribuci√≥n horaria')\n",
        "    ax.set_xlabel('Hora del d√≠a')\n",
        "    plt.suptitle('')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # promedio d√≠a-hora\n",
        "    fig, ax = plt.subplots(figsize=(7,3))\n",
        "    mean_dh = df_aux.groupby(['week_day','hour_day'])[columna].mean()\n",
        "    mean_dh.unstack(level=0).plot(ax=ax)\n",
        "    ax.set_title(f\"{columna} - Promedio por hora y d√≠a\")\n",
        "    ax.set_xlabel(\"Hora del d√≠a\")\n",
        "    ax.set_ylabel(columna)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ==== Descomposici√≥n STL m√∫ltiple y retorno de componentes ====\n",
        "def descomponer_stl(df, columna, periodos, rango=None):\n",
        "    \"\"\"\n",
        "    Descompone la serie con STL usando uno o varios periodos.\n",
        "    Retorna un dict de resultados con trend, seasonal y resid para cada periodo.\n",
        "    \"\"\"\n",
        "    serie = df[columna].dropna()\n",
        "    if rango:\n",
        "        inicio, fin = pd.to_datetime(rango[0]), pd.to_datetime(rango[1])\n",
        "        serie = serie.loc[inicio:fin]\n",
        "    if not isinstance(periodos, (list,tuple)):\n",
        "        periodos = [periodos]\n",
        "\n",
        "    resultados = {}\n",
        "    for periodo in periodos:\n",
        "        stl = STL(serie, period=periodo)\n",
        "        res = stl.fit()\n",
        "        resultados[periodo] = res\n",
        "        fig = res.plot()\n",
        "        fig.suptitle(f\"STL period={periodo}\", fontsize=12)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    return resultados\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ==== Test general de estacionariedad ====\n",
        "def test_estacionariedad(df, columna, rango=None, alpha=0.05):\n",
        "    \"\"\"\n",
        "    ADF y KPSS para toda la serie o un rango espec√≠fico, con interpretaci√≥n.\n",
        "    \"\"\"\n",
        "    if rango:\n",
        "        inicio, fin = pd.to_datetime(rango[0]), pd.to_datetime(rango[1])\n",
        "        serie = df[columna].loc[inicio:fin].dropna()\n",
        "        print(f\"\\nüìà Estacionariedad de '{columna}' entre {inicio} y {fin}\")\n",
        "    else:\n",
        "        serie = df[columna].dropna()\n",
        "        print(f\"\\nüìà Estacionariedad de toda la serie '{columna}'\")\n",
        "\n",
        "    adf = adfuller(serie)\n",
        "    kpss_ = kpss(serie, nlags=\"auto\")\n",
        "    print(f\"ADF   ‚Üí Estad√≠stico: {adf[0]:.4f}, p-value: {adf[1]:.4f} \"\n",
        "          f\"-> {'Estacionaria' if adf[1]<alpha else 'No estacionaria'}\")\n",
        "    print(f\"KPSS  ‚Üí Estad√≠stico: {kpss_[0]:.4f}, p-value: {kpss_[1]:.4f} \"\n",
        "          f\"-> {'No estacionaria' if kpss_[1]<alpha else 'Estacionaria'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5afbe8"
      },
      "outputs": [],
      "source": [
        "# Ejemplo de llamada:\n",
        "train, val, test = dividir_serie_temporal(df, 'OFF Calls', '2024-10-31 23:59:00', '2025-05-30 23:59:00')\n",
        "fig = graficar_particiones_temporales(train, val, test, 'OFF Calls', '2024-10-31 23:59:00', '2025-05-30 23:59:00')\n",
        "probar_estacionariedad(df, 'OFF Calls', alpha=0.05)\n",
        "graficar_con_zoom(df, 'OFF Calls',('2024-05-01 14:00:00', '2024-06-01 14:00:00'))\n",
        "graficar_boxplots_estacionalidad(df, 'OFF Calls')\n",
        "res_dict = descomponer_stl(df, 'OFF Calls', [48, 48*7], rango=('2024-05-01 14:00:00', '2024-06-01 14:00:00'))\n",
        "test_estacionariedad(df, 'OFF Calls', rango=('2024-05-01 14:00:00','2024-06-01 14:00:00'), alpha=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c0087"
      },
      "outputs": [],
      "source": [
        "# Handled Calls\n",
        "train, val, test = dividir_serie_temporal(df, 'Handled Calls', '2024-10-31 23:59:00', '2025-05-31 23:59:00')\n",
        "fig = graficar_particiones_temporales(train, val, test, 'Handled Calls', '2024-10-31 23:59:00', '2025-05-31 23:59:00')\n",
        "probar_estacionariedad(df, 'Handled Calls', alpha=0.05)\n",
        "graficar_con_zoom(df, 'Handled Calls', ('2024-05-01 14:00:00', '2024-06-01 14:00:00'))\n",
        "graficar_boxplots_estacionalidad(df, 'Handled Calls')\n",
        "res_dict = descomponer_stl(df, 'Handled Calls', [48, 48*7], rango=('2024-05-01 14:00:00', '2024-06-01 14:00:00'))\n",
        "test_estacionariedad(df, 'Handled Calls', rango=('2024-05-01 14:00:00', '2024-06-01 14:00:00'), alpha=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bc076"
      },
      "outputs": [],
      "source": [
        "# Real AHT\n",
        "train, val, test = dividir_serie_temporal(df, 'Real AHT', '2024-10-31 23:59:00', '2025-05-31 23:59:00')\n",
        "fig = graficar_particiones_temporales(train, val, test, 'Real AHT', '2024-10-31 23:59:00', '2025-05-31 23:59:00')\n",
        "probar_estacionariedad(df, 'Real AHT', alpha=0.05)\n",
        "graficar_con_zoom(df, 'Real AHT', ('2024-05-01 14:00:00', '2024-06-01 14:00:00'))\n",
        "graficar_boxplots_estacionalidad(df, 'Real AHT')\n",
        "res_dict = descomponer_stl(df, 'Real AHT', [48, 48*7], rango=('2024-05-01 14:00:00', '2024-06-01 14:00:00'))\n",
        "test_estacionariedad(df, 'Real AHT', rango=('2024-05-01 14:00:00', '2024-06-01 14:00:00'), alpha=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71b911"
      },
      "outputs": [],
      "source": [
        "# Real TSF\n",
        "train, val, test = dividir_serie_temporal(df, 'Real TSF', '2024-10-31 23:59:00', '2025-05-31 23:59:00')\n",
        "fig = graficar_particiones_temporales(train, val, test, 'Real TSF', '2024-10-31 23:59:00', '2025-05-31 23:59:00')\n",
        "probar_estacionariedad(df, 'Real TSF', alpha=0.05)\n",
        "graficar_con_zoom(df, 'Real TSF', ('2024-05-01 14:00:00', '2024-06-01 14:00:00'))\n",
        "graficar_boxplots_estacionalidad(df, 'Real TSF')\n",
        "res_dict = descomponer_stl(df, 'Real TSF', [48, 48*7], rango=('2024-05-01 14:00:00', '2024-06-01 14:00:00'))\n",
        "test_estacionariedad(df, 'Real TSF', rango=('2024-05-01 14:00:00', '2024-06-01 14:00:00'), alpha=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4be791"
      },
      "outputs": [],
      "source": [
        "# Real ABA\n",
        "train, val, test = dividir_serie_temporal(df, 'Real ABA', '2024-10-31 23:59:00', '2025-05-31 23:59:00')\n",
        "fig = graficar_particiones_temporales(train, val, test, 'Real ABA', '2024-10-31 23:59:00', '2025-05-31 23:59:00')\n",
        "probar_estacionariedad(df, 'Real ABA', alpha=0.05)\n",
        "graficar_con_zoom(df, 'Real ABA', ('2024-05-01 14:00:00', '2024-06-01 14:00:00'))\n",
        "graficar_boxplots_estacionalidad(df, 'Real ABA')\n",
        "res_dict = descomponer_stl(df, 'Real ABA', [48, 48*7], rango=('2024-05-01 14:00:00', '2024-06-01 14:00:00'))\n",
        "test_estacionariedad(df, 'Real ABA', rango=('2024-05-01 14:00:00', '2024-06-01 14:00:00'), alpha=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dae048"
      },
      "outputs": [],
      "source": [
        "# Real ASA\n",
        "train, val, test = dividir_serie_temporal(df, 'Real ASA', '2024-10-31 23:59:00', '2025-05-31 23:59:00')\n",
        "fig = graficar_particiones_temporales(train, val, test, 'Real ASA', '2024-10-31 23:59:00', '2025-05-31 23:59:00')\n",
        "probar_estacionariedad(df, 'Real ASA', alpha=0.05)\n",
        "graficar_con_zoom(df, 'Real ASA', ('2024-05-01 14:00:00', '2024-06-01 14:00:00'))\n",
        "graficar_boxplots_estacionalidad(df, 'Real ASA')\n",
        "res_dict = descomponer_stl(df, 'Real ASA', [48, 48*7], rango=('2024-05-01 14:00:00', '2024-06-01 14:00:00'))\n",
        "test_estacionariedad(df, 'Real ASA', rango=('2024-05-01 14:00:00', '2024-06-01 14:00:00'), alpha=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "628919"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "8010f5"
      },
      "source": [
        "#### 2. Autocorrelacion y autocorrealcion parcial de targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "096546"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from statsmodels.tsa.stattools import adfuller, kpss, acf, pacf\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "import plotly.graph_objects as go\n",
        "from skforecast.plot import set_dark_theme\n",
        "from matplotlib.gridspec import GridSpec\n",
        "\n",
        "# ==== Divisi√≥n de serie temporal sin solapamientos ====\n",
        "# ... (funciones previas mantendr√≠an su lugar)\n",
        "\n",
        "\n",
        "# ==== Autocorrelaci√≥n y Autocorrelaci√≥n Parcial ====\n",
        "def plot_autocorrelation(data, columna_objetivo, orden_diff=0, lags=120, figsize=(5, 2)):\n",
        "    \"\"\"\n",
        "    Grafica la funci√≥n de autocorrelaci√≥n (ACF) para `columna_objetivo`,\n",
        "    opcionalmente diferenciada `orden_diff` veces.\n",
        "    \"\"\"\n",
        "    serie = data[columna_objetivo].dropna()\n",
        "    if orden_diff > 0:\n",
        "        serie = serie.diff(periods=orden_diff).dropna()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    plot_acf(serie, ax=ax, lags=lags)\n",
        "    ax.set_title(f\"ACF - {columna_objetivo} (diff={orden_diff})\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_partial_autocorrelation(data, columna_objetivo, orden_diff=0, lags=120, figsize=(5, 2)):\n",
        "    \"\"\"\n",
        "    Grafica la funci√≥n de autocorrelaci√≥n parcial (PACF) para `columna_objetivo`,\n",
        "    opcionalmente diferenciada `orden_diff` veces.\n",
        "    \"\"\"\n",
        "    serie = data[columna_objetivo].dropna()\n",
        "    if orden_diff > 0:\n",
        "        serie = serie.diff(periods=orden_diff).dropna()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    plot_pacf(serie, ax=ax, lags=lags, method='ywm')\n",
        "    ax.set_title(f\"PACF - {columna_objetivo} (diff={orden_diff})\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "\n",
        "def calcular_lag_autocorrelaciones(\n",
        "    serie: pd.Series,\n",
        "    n_lags: int = 120,\n",
        "    orden_diff: int = 0,\n",
        "    sort_by: str = \"partial_autocorrelation_abs\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calcula ACF y PACF de `serie` con `n_lags`, tras diferenciar `orden_diff` veces.\n",
        "\n",
        "    Retorna DataFrame ordenado seg√∫n `sort_by`.\n",
        "    \"\"\"\n",
        "    if not isinstance(serie, pd.Series):\n",
        "        raise ValueError(\"La entrada 'serie' debe ser una pd.Series\")\n",
        "\n",
        "    s = serie.dropna()\n",
        "    if orden_diff > 0:\n",
        "        s = s.diff(periods=orden_diff).dropna()\n",
        "\n",
        "    acf_vals = acf(s, nlags=n_lags)\n",
        "    pacf_vals = pacf(s, nlags=n_lags, method='ywm')\n",
        "\n",
        "    df_lags = pd.DataFrame({\n",
        "        \"lag\": range(len(acf_vals)),\n",
        "        \"autocorrelation\": acf_vals,\n",
        "        \"partial_autocorrelation\": pacf_vals\n",
        "    })\n",
        "    df_lags[\"autocorrelation_abs\"] = df_lags[\"autocorrelation\"].abs()\n",
        "    df_lags[\"partial_autocorrelation_abs\"] = df_lags[\"partial_autocorrelation\"].abs()\n",
        "    df_lags = df_lags[df_lags[\"lag\"] > 0]\n",
        "\n",
        "    if sort_by in df_lags.columns:\n",
        "        df_lags = df_lags.sort_values(by=sort_by, ascending=False)\n",
        "\n",
        "    return df_lags.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "369dd8"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'OFF Calls', orden_diff=0, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'OFF Calls', orden_diff=0, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['OFF Calls'], n_lags=120, orden_diff=0, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ca7e8"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'OFF Calls', orden_diff=1, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'OFF Calls', orden_diff=1, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['OFF Calls'], n_lags=120, orden_diff=1, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfa034"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'OFF Calls', orden_diff=2, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'OFF Calls', orden_diff=2, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['OFF Calls'], n_lags=120, orden_diff=2, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "732b0b"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Handled Calls', orden_diff=0, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Handled Calls', orden_diff=0, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Handled Calls'], n_lags=120, orden_diff=0, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67e3ea"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Handled Calls', orden_diff=1, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Handled Calls', orden_diff=1, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Handled Calls'], n_lags=120, orden_diff=1, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8949ac"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Handled Calls', orden_diff=2, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Handled Calls', orden_diff=2, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Handled Calls'], n_lags=120, orden_diff=2, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9651e2"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Real AHT', orden_diff=0, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Real AHT', orden_diff=0, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Real AHT'], n_lags=120, orden_diff=0, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1afa5c"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Real AHT', orden_diff=1, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Real AHT', orden_diff=1, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Real AHT'], n_lags=120, orden_diff=1, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0578b2"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Real AHT', orden_diff=2, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Real AHT', orden_diff=2, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Real AHT'], n_lags=120, orden_diff=2, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07e4d0"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Real TSF', orden_diff=0, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Real TSF', orden_diff=0, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Real TSF'], n_lags=120, orden_diff=0, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0839c"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Real TSF', orden_diff=1, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Real TSF', orden_diff=1, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Real TSF'], n_lags=120, orden_diff=1, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d1c7a"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Real TSF', orden_diff=2, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Real TSF', orden_diff=2, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Real TSF'], n_lags=120, orden_diff=2, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6a7d5"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Real ABA', orden_diff=0, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Real ABA', orden_diff=0, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Real ABA'], n_lags=120, orden_diff=0, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3fd98"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Real ABA', orden_diff=1, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Real ABA', orden_diff=1, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Real ABA'], n_lags=120, orden_diff=1, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f40706"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Real ABA', orden_diff=2, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Real ABA', orden_diff=2, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Real ABA'], n_lags=120, orden_diff=2, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8079f"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Real ASA', orden_diff=0, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Real ASA', orden_diff=0, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Real ASA'], n_lags=120, orden_diff=0, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9b907"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Real ASA', orden_diff=1, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Real ASA', orden_diff=1, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Real ASA'], n_lags=120, orden_diff=1, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e906e"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Real ASA', orden_diff=2, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Real ASA', orden_diff=2, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Real ASA'], n_lags=120, orden_diff=2, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "c42203"
      },
      "source": [
        "### 3. Baseline antes de usar modelos complejos\n",
        "\n",
        "#### Seasonal Naive Forecasting and backtesting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CzsF9TuSYdW"
      },
      "outputs": [],
      "source": [
        "df.index = pd.to_datetime(df.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trAQpY-hSYdX"
      },
      "outputs": [],
      "source": [
        "df.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eabf9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from skforecast.recursive import ForecasterEquivalentDate\n",
        "from skforecast.model_selection import TimeSeriesFold, backtesting_forecaster\n",
        "\n",
        "def baseline_naive_forecast(\n",
        "    df: pd.DataFrame,\n",
        "    column: str,\n",
        "    end_validation: str,\n",
        "    offset: pd.DateOffset = pd.DateOffset(days=1),\n",
        "    steps: int = 48,\n",
        "    metric: str = 'mean_absolute_error'\n",
        "):\n",
        "    \"\"\"\n",
        "    Construye un baseline naive (valor del mismo instante del d√≠a anterior),\n",
        "    eval√∫a su MAE con backtesting y devuelve la m√©trica y las predicciones.\n",
        "    \"\"\"\n",
        "    # 1. Preparar y ajustar el forecaster\n",
        "    y = df[column].dropna()\n",
        "    y_train = y.loc[: end_validation]\n",
        "    forecaster = ForecasterEquivalentDate(\n",
        "        offset    = offset,\n",
        "        n_offsets = 1\n",
        "    )\n",
        "    forecaster.fit(y = y_train)\n",
        "\n",
        "    # 2. Configurar el CV\n",
        "    cv = TimeSeriesFold(\n",
        "        initial_train_size = len(y_train),\n",
        "        steps              = steps,\n",
        "        refit              = False\n",
        "    )\n",
        "\n",
        "    # 3. Ejecutar backtesting\n",
        "    metric_value, predictions = backtesting_forecaster(\n",
        "        forecaster = forecaster,\n",
        "        y          = y,\n",
        "        cv         = cv,\n",
        "        metric     = metric\n",
        "    )\n",
        "\n",
        "    # 4. Asegurar un valor num√©rico escalar de la m√©trica\n",
        "    try:\n",
        "        metric_float = float(metric_value)\n",
        "    except Exception:\n",
        "        # Si es Series o DataFrame, tomamos la media de sus valores\n",
        "        if hasattr(metric_value, 'mean'):\n",
        "            metric_float = metric_value.mean() if not isinstance(metric_value, pd.DataFrame) \\\n",
        "                            else metric_value.values.mean()\n",
        "        else:\n",
        "            # fallback gen√©rico\n",
        "            metric_float = sum(metric_value) / len(metric_value)\n",
        "\n",
        "    print(f\"‚úÖ Baseline naive [{column}] ‚Üí {metric.upper()} = {metric_float:.2f}\")\n",
        "    return metric_float, predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd289c"
      },
      "outputs": [],
      "source": [
        "targets = ['OFF Calls', 'Handled Calls', 'Real AHT', 'Real ABA', 'Real TSF', 'Real ASA']\n",
        "end_val = '2025-05-31 23:59:00'\n",
        "results = {}\n",
        "\n",
        "for col in targets:\n",
        "    mae, preds = baseline_naive_forecast(\n",
        "        df,\n",
        "        column         = col,\n",
        "        end_validation = end_val,\n",
        "        offset         = pd.DateOffset(days=1),\n",
        "        steps          = 48,\n",
        "        metric         = 'mean_absolute_error'\n",
        "    )\n",
        "    results[col] = mae\n",
        "\n",
        "print(\"\\nMAE summary:\")\n",
        "for col, mae in results.items():\n",
        "    print(f\" - {col:12}: {mae:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "cf2705"
      },
      "source": [
        "## 5. Empezando modelo para OFF Calls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "baa5ac"
      },
      "source": [
        "#### Multi\\-Step Recursive Forecasting with backtesting\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7341a3"
      },
      "outputs": [],
      "source": [
        "data_train, data_val, data_test = dividir_serie_temporal(df, 'OFF Calls', '2024-10-31 23:59:00', '2025-05-31 23:59:00')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5def55"
      },
      "outputs": [],
      "source": [
        "end_train = '2024-10-31 23:59:00'\n",
        "end_validation = '2025-05-31 23:59:00'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60b645"
      },
      "outputs": [],
      "source": [
        "# Create forecaster\n",
        "# ==============================================================================\n",
        "window_features = RollingFeatures(stats=[\"mean\"], window_sizes=48 * 3)\n",
        "forecaster = ForecasterRecursive(\n",
        "                 regressor       = LGBMRegressor(random_state=15926, verbose=-1),\n",
        "                 lags            = 48,\n",
        "                 window_features = window_features\n",
        "             )\n",
        "\n",
        "# Train forecaster\n",
        "# ==============================================================================\n",
        "forecaster.fit(y=df.loc[:end_validation, 'OFF Calls'])\n",
        "forecaster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3050b9"
      },
      "outputs": [],
      "source": [
        "\n",
        "cv = TimeSeriesFold(\n",
        "        steps              = 48,\n",
        "        initial_train_size = len(df.loc[:end_validation]),\n",
        "        refit              = False\n",
        ")\n",
        "# Backtesting\n",
        "# ==============================================================================\n",
        "metric, predictions = backtesting_forecaster(\n",
        "                          forecaster    = forecaster,\n",
        "                          y             = df['OFF Calls'],\n",
        "                          cv            = cv,\n",
        "                          metric        = 'mean_absolute_error',\n",
        "                          verbose       = True, # Set to False to avoid printing\n",
        "                      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec326e"
      },
      "outputs": [],
      "source": [
        "metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9f10a"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b55833"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "def obtener_muestras_semanales_por_anos(\n",
        "    df: pd.DataFrame,\n",
        "    columna_objetivo: str = 'OFF Calls',\n",
        "    anos: list = None,\n",
        "    semanas: int = 4,\n",
        "    seed: int = 42\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    Extrae muestras de longitud `semanas` semanas para la misma fecha inicio\n",
        "    (mes, d√≠a, hora, minuto), pero en diferentes a√±os especificados en `anos`,\n",
        "    ajust√°ndose al rango disponible en el DataFrame.\n",
        "\n",
        "    Par√°metros:\n",
        "    - df: DataFrame indexado en datetime.\n",
        "    - columna_objetivo: nombre de la columna a muestrear.\n",
        "    - anos: lista de a√±os (int) para generar cada muestra.\n",
        "    - semanas: duraci√≥n de cada muestra en semanas.\n",
        "    - seed: semilla para reproducibilidad.\n",
        "\n",
        "    Retorna:\n",
        "    - Listado de dicts con 'inicio', 'fin' y 'valores'.\n",
        "    \"\"\"\n",
        "    # Copiar y ordenar\n",
        "    df2 = df.copy()\n",
        "    df2.index = pd.to_datetime(df2.index)\n",
        "    df2 = df2.sort_index()\n",
        "\n",
        "    # Filtrar a√±os disponibles\n",
        "    a√±os_disponibles = sorted(df2.index.year.unique())\n",
        "    if anos is None:\n",
        "        anos = a√±os_disponibles\n",
        "    else:\n",
        "        anos = [y for y in anos if y in a√±os_disponibles]\n",
        "    if not anos:\n",
        "        raise ValueError(\"No hay a√±os disponibles en el DataFrame para las muestras solicitadas.\")\n",
        "\n",
        "    # Inferir frecuencia y duraciones\n",
        "    freq = df2.index.freq or pd.infer_freq(df2.index)\n",
        "    freq_delta = pd.to_timedelta(freq)\n",
        "    delta = pd.Timedelta(weeks=semanas)\n",
        "\n",
        "    # √çndice del primer a√±o y rango v√°lido de inicio\n",
        "    primer_ano = anos[0]\n",
        "    idx_first = df2.loc[str(primer_ano)].index\n",
        "    start_bound = idx_first.min()\n",
        "    end_bound = idx_first.max() - delta + freq_delta\n",
        "    valid_starts = idx_first[(idx_first >= start_bound) & (idx_first <= end_bound)]\n",
        "    if valid_starts.empty:\n",
        "        raise ValueError(\"No hay rangos de inicio v√°lidos para el primer a√±o.\")\n",
        "\n",
        "    # Seleccionar un inicio que exista en todos los a√±os\n",
        "    random.seed(seed)\n",
        "    candidate = None\n",
        "    for _ in range(1000):\n",
        "        ts = random.choice(valid_starts)\n",
        "        fin = ts + delta - freq_delta\n",
        "        if all(\n",
        "            (ts.replace(year=y) >= df2.loc[str(y)].index.min()) and\n",
        "            (fin.replace(year=y) <= df2.loc[str(y)].index.max())\n",
        "            for y in anos\n",
        "        ):\n",
        "            candidate = ts\n",
        "            break\n",
        "    if candidate is None:\n",
        "        raise ValueError(\"No se encontr√≥ inicio v√°lido tras m√∫ltiples intentos.\")\n",
        "\n",
        "    # Generar muestras para cada a√±o\n",
        "    muestras = []\n",
        "    for y in anos:\n",
        "        inicio = candidate.replace(year=y)\n",
        "        fin = inicio + delta - freq_delta\n",
        "        valores = df2.loc[inicio:fin, columna_objetivo].tolist()\n",
        "        muestras.append({'inicio': inicio, 'fin': fin, 'valores': valores})\n",
        "\n",
        "    return muestras\n",
        "\n",
        "\n",
        "def mostrar_muestras(muestras: list) -> None:\n",
        "    \"\"\"\n",
        "    Imprime cada muestra con detalles de fechas y valores.\n",
        "    \"\"\"\n",
        "    for i, m in enumerate(muestras, 1):\n",
        "        print(f\"üì¶ Muestra {i}\")\n",
        "        print(f\"   ‚è±Ô∏è Desde: {m['inicio']}  ‚Üí  Hasta: {m['fin']}\")\n",
        "        print(f\"   üî¢ Valores (primeros 10): {m['valores'][:10]}\")\n",
        "        print(f\"   üî¢ Valores (todos): {m['valores']}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93c19f"
      },
      "outputs": [],
      "source": [
        "# Ejemplo:\n",
        "muestras = obtener_muestras_semanales_por_anos(\n",
        "    df,\n",
        "    columna_objetivo='OFF Calls',\n",
        "    anos=[2023,2024,2025],\n",
        "    semanas=4,\n",
        "    seed=42\n",
        ")\n",
        "mostrar_muestras(muestras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e8d16"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from feature_engine.datetime import DatetimeFeatures\n",
        "from feature_engine.creation import CyclicalFeatures\n",
        "# (Optional) define holiday list as in original\n",
        "\n",
        "def crear_variables_exogenas_callcenter(df: pd.DataFrame, columna_objetivo: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Crea variables ex√≥genas para predicci√≥n de OFF Calls en un call center (frecuencia 30-min).\n",
        "    Incluye:\n",
        "    - Calendario y codificaci√≥n c√≠clica\n",
        "    - Feriados en Jamaica (2021‚Äì2025)\n",
        "    - Variables sem√°nticas (fin de semana, hora laboral, pico de horas)\n",
        "    - Estad√≠sticas rodantes sobre el objetivo (media, max, std, CV)\n",
        "    - Indicadores de picos y zonas de trabajo (clusters)\n",
        "\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    y = df[columna_objetivo]\n",
        "\n",
        "    # 1. Calendario\n",
        "    calendar = DatetimeFeatures(\n",
        "        variables='index',\n",
        "        features_to_extract=['month', 'week', 'day_of_week', 'hour'],\n",
        "        drop_original=True\n",
        "    ).fit_transform(df)\n",
        "\n",
        "    # 2. C√≠clico\n",
        "    cyc = CyclicalFeatures(\n",
        "        variables=['month','week','day_of_week','hour'],\n",
        "        max_values={'month':12,'week':52,'day_of_week':6,'hour':24},\n",
        "        drop_original=False\n",
        "    ).fit_transform(calendar)\n",
        "\n",
        "    # 3. Feriados (simplificado al 2021-2025)\n",
        "    feriados = pd.to_datetime([\n",
        "        '2021-01-01',  # New Year's Day\n",
        "        '2021-02-17',  # Ash Wednesday\n",
        "        '2021-04-02',  # Good Friday\n",
        "        '2021-04-04',  # Easter Day\n",
        "        '2021-04-05',  # Easter Monday\n",
        "        '2021-05-24',  # Labour Day (in lieu)\n",
        "        '2021-08-01',  # Emancipation Day\n",
        "        '2021-08-02',  # Emancipation Day (in lieu)\n",
        "        '2021-08-06',  # Independence Day\n",
        "        '2021-10-18',  # National Heroes' Day\n",
        "        '2021-12-25',  # Christmas Day\n",
        "        '2021-12-26',  # Boxing Day\n",
        "        '2021-12-27',  # Christmas Day (in lieu)\n",
        "        '2022-01-01',  # New Year's Day\n",
        "        '2022-03-02',  # Ash Wednesday\n",
        "        '2022-04-15',  # Good Friday\n",
        "        '2022-04-17',  # Easter Day\n",
        "        '2022-04-18',  # Easter Monday\n",
        "        '2022-05-23',  # Labour Day\n",
        "        '2022-08-01',  # Emancipation Day\n",
        "        '2022-08-06',  # Independence Day\n",
        "        '2022-10-17',  # National Heroes' Day\n",
        "        '2022-12-25',  # Christmas Day\n",
        "        '2022-12-26',  # Christmas Day (in lieu)\n",
        "        '2022-12-27',  # Boxing Day (in lieu)\n",
        "        '2023-01-01',  # New Year's Day\n",
        "        '2023-02-22',  # Ash Wednesday\n",
        "        '2023-04-07',  # Good Friday\n",
        "        '2023-04-09',  # Easter Day\n",
        "        '2023-04-10',  # Easter Monday\n",
        "        '2023-05-23',  # Labour Day\n",
        "        '2023-08-01',  # Emancipation Day\n",
        "        '2023-08-06',  # Independence Day\n",
        "        '2023-08-07',  # Independence Day (in lieu)\n",
        "        '2023-10-16',  # National Heroes' Day\n",
        "        '2023-12-25',  # Christmas Day\n",
        "        '2023-12-26',  # Boxing Day\n",
        "        '2024-01-01',  # New Year's Day\n",
        "        '2024-02-14',  # Ash Wednesday\n",
        "        '2024-03-29',  # Good Friday\n",
        "        '2024-04-01',  # Easter Monday\n",
        "        '2024-05-23',  # Labour Day\n",
        "        '2024-08-01',  # Emancipation Day\n",
        "        '2024-08-06',  # Independence Day\n",
        "        '2024-10-21',  # National Heroes' Day\n",
        "        '2024-12-25',  # Christmas Day\n",
        "        '2024-12-26',  # Boxing Day\n",
        "        '2025-01-01',  # New Year's Day\n",
        "        '2025-03-05',  # Ash Wednesday\n",
        "        '2025-04-18',  # Good Friday\n",
        "        '2025-04-21',  # Easter Monday\n",
        "        '2025-05-23',  # Labour Day\n",
        "        '2025-08-01',  # Emancipation Day\n",
        "        '2025-08-06',  # Independence Day\n",
        "        '2025-10-21',  # National Heroes' Day\n",
        "        '2025-12-25',  # Christmas Day\n",
        "        '2025-12-26',  # Boxing Day\n",
        "        '2026-01-01',  # New Year's Day\n",
        "        '2026-02-18',  # Ash Wednesday\n",
        "        '2026-04-03',  # Good Friday\n",
        "        '2026-04-06',  # Easter Monday\n",
        "        '2026-05-25',  # Labour Day (observed)\n",
        "        '2026-08-01',  # Emancipation Day\n",
        "        '2026-08-06',  # Independence Day\n",
        "        '2026-10-19',  # National Heroes' Day\n",
        "        '2026-12-25',  # Christmas Day\n",
        "        '2026-12-26',  # Boxing Day\n",
        "        '2027-02-10',  # Ash Wednesday\n",
        "        '2027-03-26',  # Good Friday\n",
        "        '2027-03-29',  # Easter Monday\n",
        "        '2027-05-24',  # Labour Day (observed)\n",
        "        '2027-08-02',  # Emancipation Day (observed)\n",
        "        '2027-08-06',  # Independence Day\n",
        "        '2027-10-18',  # National Heroes' Day\n",
        "        '2027-12-25',  # Christmas Day\n",
        "        '2027-12-26',  # Boxing Day\n",
        "        '2027-12-27',  # Boxing Day (substitute)\n",
        "        '2028-01-01',  # New Year's Day\n",
        "        '2028-03-01',  # Ash Wednesday\n",
        "        '2028-04-14',  # Good Friday\n",
        "        '2028-04-17',  # Easter Monday\n",
        "        '2028-05-23',  # Labour Day\n",
        "        '2028-08-01',  # Emancipation Day\n",
        "        '2028-08-06',  # Independence Day\n",
        "        '2028-10-16',  # National Heroes' Day\n",
        "        '2028-12-25',  # Christmas Day\n",
        "        '2028-12-26',  # Boxing Day\n",
        "\n",
        "    ])\n",
        "    holiday = df.index.normalize().isin(feriados).astype(int)\n",
        "    hol = pd.DataFrame({'holiday':holiday}, index=df.index)\n",
        "    hol['prev_holiday'] = hol['holiday'].shift(48).fillna(0).astype(int)\n",
        "    hol['next_holiday'] = hol['holiday'].shift(-48).fillna(0).astype(int)\n",
        "\n",
        "    # 4. Estad√≠sticas rodantes (rolling) SIN generar NaN en la primera fila\n",
        "    windows = {'1D':48, '3D':48*3, '7D':48*7}\n",
        "    roll = pd.DataFrame(index=df.index)\n",
        "    for label, size in windows.items():\n",
        "        roll[f'roll_mean_{label}'] = y.rolling(window=size, min_periods=1).mean()\n",
        "        roll[f'roll_max_{label}']  = y.rolling(window=size, min_periods=1).max()\n",
        "        # ddof=0 para que std con un solo punto sea 0, no NaN\n",
        "        roll[f'roll_std_{label}']  = (\n",
        "            y.rolling(window=size, min_periods=1)\n",
        "             .std(ddof=0)\n",
        "        )\n",
        "        # coeficiente de variaci√≥n, sin infinitos ni NaN\n",
        "        roll[f'roll_cv_{label}']   = (\n",
        "            roll[f'roll_std_{label}']\n",
        "            .div( roll[f'roll_mean_{label}'].replace(0, np.nan) )\n",
        "            .fillna(0)\n",
        "        )\n",
        "\n",
        "    # 5. Indicadores de picos y zonas\n",
        "    q33, q66, q95 = np.percentile(y.dropna(), [33, 66, 95])\n",
        "    zone = pd.cut(y, bins=[-np.inf, q33, q66, np.inf], labels=[0,1,2]).astype(int)\n",
        "    peak = (y >= q95).astype(int)\n",
        "    peaks = pd.DataFrame({'zone_cluster':zone, 'is_peak_event':peak}, index=df.index)\n",
        "\n",
        "    # 6. Variables sem√°nticas\n",
        "    logic = pd.DataFrame(index=df.index)\n",
        "    logic['is_weekend']      = df.index.dayofweek.isin([5,6]).astype(int)\n",
        "    logic['is_working_hour'] = ((df.index.hour>=7)&(df.index.hour<18)).astype(int)\n",
        "    logic['is_peak_hour']    = ((df.index.hour>=9)&(df.index.hour<16)).astype(int)\n",
        "\n",
        "    # 7. Concatenar\n",
        "    exog = pd.concat([cyc, hol, roll, peaks, logic], axis=1)\n",
        "\n",
        "    # 8. Eliminar todas las columnas que vinieran de df (incluyendo otros targets)\n",
        "    original_cols = df.columns.tolist()\n",
        "    exog = exog.drop(columns=original_cols, errors='ignore')\n",
        "\n",
        "    # Si quieres asegurarte de no generar NaNs imprevistos:\n",
        "    exog = exog.fillna(0)\n",
        "    return exog\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb1c05"
      },
      "outputs": [],
      "source": [
        "# 1. Importa (si est√° en otro fichero):\n",
        "# from sampling_utils import crear_variables_exogenas_callcenter\n",
        "\n",
        "# 2. Llama a la funci√≥n pas√°ndole tu DataFrame y la columna objetivo\n",
        "exog = crear_variables_exogenas_callcenter(\n",
        "    df=df,\n",
        "    columna_objetivo='OFF Calls'\n",
        ")\n",
        "\n",
        "# 3. Comprueba el resultado\n",
        "print(f\"Shape de ex√≥genas: {exog.shape}\")\n",
        "display(exog.head(80000))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8ceed"
      },
      "outputs": [],
      "source": [
        "df.head(80000)\n",
        "print(f\"Shape de data: {df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c59be"
      },
      "outputs": [],
      "source": [
        "df.head(80000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6407e"
      },
      "outputs": [],
      "source": [
        "print(exog.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edfc4c"
      },
      "outputs": [],
      "source": [
        "print('OFF Calls' in exog.columns)  # ‚Üí False esperado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "053e46"
      },
      "source": [
        "Estas s√≠ las conoces por adelantado. Por ejemplo:\n",
        "- Volumen planificado: turnos por agente\n",
        "- Carga anticipada: reservas, sesiones agendadas, campa√±as\n",
        "- M√©tricas hist√≥ricas programadas: si sabes la cantidad de contactos estimados, o el canal m√°s usado\n",
        "- Condiciones repetitivas: si cada lunes a las 9am hay saturaci√≥n, puedes modelarlo con calendario + lag.\n",
        "‚öôÔ∏è O usar lags autoregresiv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c30419"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "def crear_interacciones_exogenas(\n",
        "    exog: pd.DataFrame,\n",
        "    columnas_interaccion: list[str],\n",
        "    degree: int = 2\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    A√±ade interacciones de segundo orden (solo t√©rminos cruzados)\n",
        "    a un DataFrame de variables ex√≥genas.\n",
        "\n",
        "    Par√°metros\n",
        "    ----------\n",
        "    exog : pd.DataFrame\n",
        "        DataFrame con variables ex√≥genas indexado en datetime.\n",
        "    columnas_interaccion : list[str]\n",
        "        Lista de columnas de `exog` sobre las que generar interacciones.\n",
        "    degree : int, opcional\n",
        "        Grado del polinomio; por defecto 2 (interacciones de segundo orden).\n",
        "\n",
        "    Retorna\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame original concatenado con las nuevas caracter√≠sticas de interacci√≥n.\n",
        "    \"\"\"\n",
        "    # Configurar el transformador para que devuelva un DataFrame de pandas\n",
        "    transformer = PolynomialFeatures(\n",
        "        degree=degree,\n",
        "        interaction_only=True,\n",
        "        include_bias=False\n",
        "    )\n",
        "    transformer.set_output(transform=\"pandas\")\n",
        "\n",
        "    # Ajustar y transformar solo las columnas seleccionadas\n",
        "    df_poly = transformer.fit_transform(exog[columnas_interaccion])\n",
        "\n",
        "    # Eliminar las columnas originales si no las queremos duplicadas\n",
        "    df_poly = df_poly.drop(columns=columnas_interaccion, errors='ignore')\n",
        "\n",
        "    # Renombrar las columnas de interacci√≥n para identificarlas\n",
        "    df_poly.columns = [\n",
        "        f\"poly__{col.replace(' ', '__')}\" for col in df_poly.columns\n",
        "    ]\n",
        "    df_poly.index = exog.index\n",
        "\n",
        "    # Concatenar con el DataFrame original\n",
        "    exog_expanded = pd.concat([exog, df_poly], axis=1)\n",
        "    return exog_expanded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "929be2"
      },
      "outputs": [],
      "source": [
        "# 1. Define las columnas sobre las que quieres interacciones\n",
        "poly_cols = [\n",
        "    'month_sin','month_cos',\n",
        "    'week_sin','week_cos',\n",
        "    'day_of_week_sin','day_of_week_cos',\n",
        "    'hour_sin','hour_cos',\n",
        "    'holiday','prev_holiday','next_holiday',\n",
        "    'roll_mean_1D','roll_max_1D','roll_std_1D','roll_cv_1D',\n",
        "    'roll_mean_3D','roll_max_3D','roll_std_3D','roll_cv_3D',\n",
        "    'roll_mean_7D','roll_max_7D','roll_std_7D','roll_cv_7D',\n",
        "    'zone_cluster','is_peak_event',\n",
        "    'is_weekend','is_working_hour','is_peak_hour'\n",
        "]\n",
        "\n",
        "# 2. Aplica la funci√≥n\n",
        "exog_con_interacciones = crear_interacciones_exogenas(exog, poly_cols)\n",
        "\n",
        "# 3. Verifica el resultado\n",
        "print(exog_con_interacciones.shape)\n",
        "print([c for c in exog_con_interacciones.columns if c.startswith('poly__')][:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9249b"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "from lightgbm import LGBMRegressor\n",
        "from skforecast.model_selection import backtesting_forecaster\n",
        "\n",
        "# ‚úÖ 1. Limpiar columnas ex√≥genas (eliminar duplicados y mantener el orden)\n",
        "exog_features = list(OrderedDict.fromkeys(exog_features))\n",
        "\n",
        "# ‚úÖ 2. Verificar que todas existan en exog_con_interacciones\n",
        "exog_features = [col for col in exog_features if col in exog_con_interacciones.columns]\n",
        "\n",
        "# ‚úÖ 3. Construir DataFrame unificado (merge por √≠ndice)\n",
        "data = df[['OFF Calls']].merge(\n",
        "    exog_con_interacciones[exog_features],\n",
        "    left_index=True,\n",
        "    right_index=True,\n",
        "    how='inner').astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b62669"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ 4. Definir fechas de corte (ajusta si usas variables distintas)\n",
        "end_train = '2024-04-30 23:59:00'\n",
        "end_validation = '2025-03-31 23:59:00'\n",
        "\n",
        "# ‚úÖ 5. Dividir en train / val / test\n",
        "data_train = data.loc[:end_train, :].copy()\n",
        "data_val   = data.loc[end_train:end_validation, :].copy()\n",
        "data_test  = data.loc[end_validation:, :].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00bef4"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c5951"
      },
      "outputs": [],
      "source": [
        "# Redefinir el forecaster si no se ha hecho con las nuevas series\n",
        "window_features = RollingFeatures(stats=[\"mean\"], window_sizes=48*3)\n",
        "\n",
        "forecaster = ForecasterRecursive(\n",
        "                 regressor       = LGBMRegressor(random_state=15926, verbose=-1),\n",
        "                 lags            = 48,\n",
        "                 window_features = window_features\n",
        "             )\n",
        "\n",
        "# Entrenamiento usando hasta end_validation (train + val)\n",
        "forecaster.fit(\n",
        "    y    = data.loc[:end_validation, 'OFF Calls'],\n",
        "    exog = data.loc[:end_validation, exog_features]\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25918c"
      },
      "outputs": [],
      "source": [
        "# Configurar folds temporales\n",
        "cv = TimeSeriesFold(\n",
        "    steps              = 48,  # horizonte de predicci√≥n\n",
        "    initial_train_size = len(data.loc[:end_validation]),\n",
        "    refit              = False\n",
        ")\n",
        "\n",
        "# Ejecutar backtesting\n",
        "metric, predictions = backtesting_forecaster(\n",
        "    forecaster = forecaster,\n",
        "    y          = data['OFF Calls'],\n",
        "    exog       = data[exog_features],\n",
        "    cv         = cv,\n",
        "    metric     = 'mean_absolute_error',\n",
        "    verbose    = True\n",
        ")\n",
        "\n",
        "# Visualizar resultados\n",
        "display(metric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e05b6b"
      },
      "outputs": [],
      "source": [
        "predictions.head(80000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14b9cb"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objs as go\n",
        "\n",
        "# Aseg√∫rate de que las predicciones est√©n alineadas con el √≠ndice\n",
        "fig = go.Figure()\n",
        "\n",
        "# Trazar valores reales\n",
        "fig.add_trace(go.Scatter(\n",
        "    x = data_test.index,\n",
        "    y = data_test['OFF Calls'],\n",
        "    mode = 'lines',\n",
        "    name = 'Valor real'\n",
        "))\n",
        "\n",
        "# Trazar predicciones (el √≠ndice de `predictions` ya est√° alineado)\n",
        "fig.add_trace(go.Scatter(\n",
        "    x = predictions.index,\n",
        "    y = predictions['pred'],\n",
        "    mode = 'lines',\n",
        "    name = 'Predicci√≥n',\n",
        "    line = dict(color='firebrick')\n",
        "))\n",
        "\n",
        "# Configurar layout\n",
        "fig.update_layout(\n",
        "    title = 'üìä Predicci√≥n vs Real ‚Äî OFF Calls',\n",
        "    xaxis_title = 'Fecha y hora',\n",
        "    yaxis_title = 'N√∫mero de llamadas',\n",
        "    width = 900,\n",
        "    height = 450,\n",
        "    legend = dict(orientation=\"h\", y=1.02, x=0),\n",
        "    margin = dict(l=40, r=40, t=40, b=40)\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f4bc5"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters search\n",
        "# ==============================================================================\n",
        "forecaster = ForecasterRecursive(\n",
        "                 regressor       = LGBMRegressor(random_state=15926, verbose=-1),\n",
        "                 lags            = 48,  # This value will be replaced in the grid search\n",
        "                 window_features = window_features\n",
        "             )\n",
        "\n",
        "# Lags used as predictors\n",
        "lags_grid = [48, (1, 2, 3, 4, 49, 48, 5, 50, 34)]\n",
        "\n",
        "# Regressor hyperparameters search space\n",
        "def search_space(trial):\n",
        "    search_space  = {\n",
        "        'n_estimators' : trial.suggest_int('n_estimators', 300, 1000, step=100),\n",
        "        'max_depth'    : trial.suggest_int('max_depth', 3, 10),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5),\n",
        "        'reg_alpha'    : trial.suggest_float('reg_alpha', 0, 1),\n",
        "        'reg_lambda'   : trial.suggest_float('reg_lambda', 0, 1),\n",
        "        'lags'         : trial.suggest_categorical('lags', lags_grid)\n",
        "    }\n",
        "    return search_space\n",
        "\n",
        "# Folds training and validation\n",
        "cv_search = TimeSeriesFold(\n",
        "                steps              = 48,\n",
        "                initial_train_size = len(data[:end_train]),\n",
        "                refit              = False,\n",
        "            )\n",
        "\n",
        "results_search, frozen_trial = bayesian_search_forecaster(\n",
        "                                   forecaster   = forecaster,\n",
        "                                   y            = data.loc[:end_validation, 'OFF Calls'],\n",
        "                                   exog         = data.loc[:end_validation, exog_features],\n",
        "                                   cv           = cv_search,\n",
        "                                   metric       = 'mean_absolute_error',\n",
        "                                   search_space = search_space,\n",
        "                                   n_trials     = 10,  # Increase for more exhaustive search\n",
        "                                   return_best  = True\n",
        "                               )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84166e"
      },
      "outputs": [],
      "source": [
        "# Search results\n",
        "# ==============================================================================\n",
        "best_params = results_search.at[0, 'params']\n",
        "best_params = best_params | {'random_state': 15926, 'verbose': -1}\n",
        "best_lags = results_search.at[0, 'lags']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bf9b7"
      },
      "outputs": [],
      "source": [
        "results_search.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c6978"
      },
      "outputs": [],
      "source": [
        "# Best model\n",
        "# ==============================================================================\n",
        "forecaster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44dd1e"
      },
      "outputs": [],
      "source": [
        "# Backtest final model on test data\n",
        "# ==============================================================================\n",
        "metric, predictions = backtesting_forecaster(\n",
        "                          forecaster = forecaster,\n",
        "                          y          = data['OFF Calls'],\n",
        "                          exog       = data[exog_features],\n",
        "                          cv         = cv,\n",
        "                          metric     = 'mean_absolute_error'\n",
        "                      )\n",
        "display(metric)\n",
        "predictions.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc155c"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objs as go\n",
        "\n",
        "# Aseg√∫rate de que las predicciones est√©n alineadas con el √≠ndice\n",
        "fig = go.Figure()\n",
        "\n",
        "# Trazar valores reales\n",
        "fig.add_trace(go.Scatter(\n",
        "    x = data_test.index,\n",
        "    y = data_test['OFF Calls'],\n",
        "    mode = 'lines',\n",
        "    name = 'Valor real'\n",
        "))\n",
        "\n",
        "# Trazar predicciones (el √≠ndice de `predictions` ya est√° alineado)\n",
        "fig.add_trace(go.Scatter(\n",
        "    x = predictions.index,\n",
        "    y = predictions['pred'],\n",
        "    mode = 'lines',\n",
        "    name = 'Predicci√≥n',\n",
        "    line = dict(color='firebrick')\n",
        "))\n",
        "\n",
        "# Configurar layout\n",
        "fig.update_layout(\n",
        "    title = 'üìä Predicci√≥n vs Real ‚Äî OFF Calls',\n",
        "    xaxis_title = 'Fecha y hora',\n",
        "    yaxis_title = 'N√∫mero de llamadas',\n",
        "    width = 900,\n",
        "    height = 450,\n",
        "    legend = dict(orientation=\"h\", y=1.02, x=0),\n",
        "    margin = dict(l=40, r=40, t=40, b=40)\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "c42750"
      },
      "source": [
        "### Selecci√≥n de caracteristicas\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10840b"
      },
      "outputs": [],
      "source": [
        "# Create forecaster\n",
        "# ==============================================================================\n",
        "regressor = LGBMRegressor(\n",
        "                n_estimators = 100,\n",
        "                max_depth    = 4,\n",
        "                random_state = 15926,\n",
        "                verbose      = -1\n",
        "            )\n",
        "\n",
        "forecaster = ForecasterRecursive(\n",
        "                 regressor       = regressor,\n",
        "                 lags            = best_lags,\n",
        "                 window_features = window_features\n",
        "             )\n",
        "\n",
        "# Recursive feature elimination with cross-validation\n",
        "# ==============================================================================\n",
        "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names.*\")\n",
        "selector = RFECV(\n",
        "    estimator = regressor,\n",
        "    step      = 1,\n",
        "    cv        = 3,\n",
        ")\n",
        "lags_select, window_features_select, exog_select = select_features(\n",
        "    forecaster      = forecaster,\n",
        "    selector        = selector,\n",
        "    y               = data_train['OFF Calls'],\n",
        "    exog            = data_train[exog_features],\n",
        "    select_only     = None,\n",
        "    force_inclusion = None,\n",
        "    subsample       = 0.5,  # Subsample to speed up the process\n",
        "    random_state    = 123,\n",
        "    verbose         = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1ff9e"
      },
      "outputs": [],
      "source": [
        "# Create a forecaster with the selected features\n",
        "# ==============================================================================\n",
        "forecaster = ForecasterRecursive(\n",
        "                regressor       = LGBMRegressor(**best_params),\n",
        "                lags            = lags_select,\n",
        "                window_features = window_features\n",
        "             )\n",
        "# Backtesting model with exogenous variables on test data\n",
        "# ==============================================================================\n",
        "metric, predictions = backtesting_forecaster(\n",
        "                            forecaster = forecaster,\n",
        "                            y          = data['OFF Calls'],\n",
        "                            exog       = data[exog_select],\n",
        "                            cv         = cv,\n",
        "                            metric     = 'mean_absolute_error'\n",
        "                      )\n",
        "metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "aa05c8"
      },
      "source": [
        "### Pronostico Probabilistico: Prediccion de los intervalos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b9c92"
      },
      "outputs": [],
      "source": [
        "# Create and train forecaster\n",
        "# ==============================================================================\n",
        "forecaster = ForecasterRecursive(\n",
        "                 regressor       = LGBMRegressor(**best_params),\n",
        "                 lags            = lags_select,\n",
        "                 window_features = window_features,\n",
        "                 binner_kwargs   = {\"n_bins\": 5}\n",
        "             )\n",
        "forecaster.fit(\n",
        "    y    = data.loc[:end_train, 'OFF Calls'],\n",
        "    exog = data.loc[:end_train, exog_select],\n",
        "    store_in_sample_residuals = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63ab04"
      },
      "outputs": [],
      "source": [
        "# Predict intervals\n",
        "# ==============================================================================\n",
        "# Since the model has been trained with exogenous variables, they must be provided\n",
        "# for the prediction.\n",
        "predictions = forecaster.predict_interval(\n",
        "                  exog     = data.loc[end_train:, exog_select],\n",
        "                  steps    = 48,\n",
        "                  interval = [5, 95],\n",
        "                  method  = 'conformal',\n",
        "              )\n",
        "predictions.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1e4f0"
      },
      "outputs": [],
      "source": [
        "# Backtesting on validation data to obtain out-sample residuals\n",
        "# ==============================================================================\n",
        "cv = TimeSeriesFold(\n",
        "        steps              = 48,\n",
        "        initial_train_size = len(data.loc[:end_train]),\n",
        "        refit              = False,\n",
        ")\n",
        "_, predictions_val = backtesting_forecaster(\n",
        "                         forecaster = forecaster,\n",
        "                         y          = data.loc[:end_validation, 'OFF Calls'],\n",
        "                         exog       = data.loc[:end_validation, exog_select],\n",
        "                         cv         = cv,\n",
        "                         metric     = 'mean_absolute_error'\n",
        "                     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daa8e6"
      },
      "outputs": [],
      "source": [
        "# Out-sample residuals distribution\n",
        "# ==============================================================================\n",
        "residuals = data.loc[predictions_val.index, 'OFF Calls'] - predictions_val['pred']\n",
        "print(pd.Series(np.where(residuals < 0, 'negative', 'positive')).value_counts())\n",
        "plt.rcParams.update({'font.size': 8})\n",
        "_ = plot_residuals(residuals=residuals, figsize=(7, 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efc254"
      },
      "outputs": [],
      "source": [
        "# Store out-sample residuals in the forecaster\n",
        "# ==============================================================================\n",
        "forecaster.set_out_sample_residuals(\n",
        "    y_true = data.loc[predictions_val.index, 'OFF Calls'],\n",
        "    y_pred = predictions_val['pred']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5241d"
      },
      "outputs": [],
      "source": [
        "# Backtesting with prediction intervals in test data using out-sample residuals\n",
        "# ==============================================================================\n",
        "cv = TimeSeriesFold(\n",
        "        steps              = 48,\n",
        "        initial_train_size = len(data.loc[:end_validation]),\n",
        "        refit              = False,\n",
        ")\n",
        "metric, predictions = backtesting_forecaster(\n",
        "                            forecaster              = forecaster,\n",
        "                            y                       = data['OFF Calls'],\n",
        "                            exog                    = data[exog_select],\n",
        "                            cv                      = cv,\n",
        "                            metric                  = 'mean_absolute_error',\n",
        "                            interval                = [5, 95],\n",
        "                            interval_method         = 'conformal',\n",
        "                            use_in_sample_residuals = False,  # out-sample residuals\n",
        "                            use_binned_residuals    = True,   # Intervals conditioned to the predicted values\n",
        "                       )\n",
        "predictions.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "734253"
      },
      "outputs": [],
      "source": [
        "# Plot prediction intervals vs real value\n",
        "# ==============================================================================\n",
        "fig = go.Figure([\n",
        "    go.Scatter(\n",
        "        name='Prediction', x=predictions.index, y=predictions['pred'], mode='lines',\n",
        "    ),\n",
        "    go.Scatter(\n",
        "        name='Real value', x=data_test.index, y=data_test['OFF Calls'], mode='lines',\n",
        "    ),\n",
        "    go.Scatter(\n",
        "        name='Upper Bound', x=predictions.index, y=predictions['upper_bound'],\n",
        "        mode='lines', marker=dict(color=\"#444\"), line=dict(width=0), showlegend=False\n",
        "    ),\n",
        "    go.Scatter(\n",
        "        name='Lower Bound', x=predictions.index, y=predictions['lower_bound'],\n",
        "        marker=dict(color=\"#444\"), line=dict(width=0), mode='lines',\n",
        "        fillcolor='rgba(68, 68, 68, 0.3)', fill='tonexty', showlegend=False\n",
        "    )\n",
        "])\n",
        "fig.update_layout(\n",
        "    title=\"Real value vs predicted in test data\",\n",
        "    xaxis_title=\"Date time\",\n",
        "    yaxis_title=\"OFF Calls\",\n",
        "    width=800,\n",
        "    height=400,\n",
        "    margin=dict(l=20, r=20, t=35, b=20),\n",
        "    hovermode=\"x\",\n",
        "    legend=dict(orientation=\"h\", yanchor=\"top\", y=1.1, xanchor=\"left\", x=0.001)\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40b60e"
      },
      "outputs": [],
      "source": [
        "# Predicted interval coverage (on test data)\n",
        "# ==============================================================================\n",
        "coverage = calculate_coverage(\n",
        "              y_true       = data.loc[end_validation:, \"OFF Calls\"],\n",
        "              lower_bound  = predictions[\"lower_bound\"],\n",
        "              upper_bound  = predictions[\"upper_bound\"]\n",
        "           )\n",
        "area = (predictions['upper_bound'] - predictions['lower_bound']).sum()\n",
        "print(f\"Total area of the interval: {round(area, 2)}\")\n",
        "print(f\"Predicted interval coverage: {round(100 * coverage, 2)} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "c42cf3"
      },
      "source": [
        "#### Explicabilidad e interpretabilidad del modelo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76fcad"
      },
      "outputs": [],
      "source": [
        "# Create and train forecaster\n",
        "# ==============================================================================\n",
        "forecaster = ForecasterRecursive(\n",
        "                 regressor       = LGBMRegressor(**best_params),\n",
        "                 lags            = lags_select,\n",
        "                 window_features = window_features\n",
        "             )\n",
        "forecaster.fit(\n",
        "    y    = data.loc[:end_validation, 'OFF Calls'],\n",
        "    exog = data.loc[:end_validation, exog_select]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "363473"
      },
      "outputs": [],
      "source": [
        "# Model-specific feature importances\n",
        "# ==============================================================================\n",
        "feature_importances = forecaster.get_feature_importances()\n",
        "feature_importances.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5ae1ff"
      },
      "source": [
        "#### Shap Values\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa203f"
      },
      "outputs": [],
      "source": [
        "# Training matrices used by the forecaster to fit the internal regressor\n",
        "# ==============================================================================\n",
        "X_train, y_train = forecaster.create_train_X_y(\n",
        "                       y    = data_train['OFF Calls'],\n",
        "                       exog = data_train[exog_select]\n",
        "                   )\n",
        "display(X_train.head(3))\n",
        "display(y_train.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "280f07"
      },
      "outputs": [],
      "source": [
        "# Create SHAP explainer (for three base models)\n",
        "# ==============================================================================\n",
        "shap.initjs()\n",
        "explainer = shap.TreeExplainer(forecaster.regressor)\n",
        "\n",
        "# Sample 50% of the data to speed up the calculation\n",
        "rng = np.random.default_rng(seed=785412)\n",
        "sample = rng.choice(X_train.index, size=int(len(X_train)*0.5), replace=False)\n",
        "X_train_sample = X_train.loc[sample, :]\n",
        "shap_values = explainer.shap_values(X_train_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad1767"
      },
      "outputs": [],
      "source": [
        "# Shap summary plot (top 10)\n",
        "# ==============================================================================\n",
        "shap.summary_plot(shap_values, X_train_sample, max_display=10, show=False)\n",
        "fig, ax = plt.gcf(), plt.gca()\n",
        "ax.set_title(\"SHAP Summary plot\")\n",
        "ax.tick_params(labelsize=8)\n",
        "fig.set_size_inches(6, 4.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04d9df"
      },
      "outputs": [],
      "source": [
        "# Backtesting returning the predictors\n",
        "# ==============================================================================\n",
        "cv = TimeSeriesFold(\n",
        "        steps              = 48,\n",
        "        initial_train_size = len(data.loc[:end_validation]),\n",
        ")\n",
        "_, predictions = backtesting_forecaster(\n",
        "                        forecaster        = forecaster,\n",
        "                        y                 = data['OFF Calls'],\n",
        "                        exog              = data[exog_select],\n",
        "                        cv                = cv,\n",
        "                        metric            = 'mean_absolute_error',\n",
        "                        return_predictors = True,\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dcc41"
      },
      "outputs": [],
      "source": [
        "predictions.head(8000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbd990"
      },
      "outputs": [],
      "source": [
        "# Waterfall for a single prediction generated during backtesting\n",
        "# ==============================================================================\n",
        "predictions = predictions.astype(data[exog_select].dtypes) # Ensure that the types are the same\n",
        "iloc_predicted_date = predictions.index.get_loc('2025-04-01 21:30:00')\n",
        "shap_values_single = explainer(predictions.iloc[:, 2:])\n",
        "shap.plots.waterfall(shap_values_single[iloc_predicted_date], show=False)\n",
        "fig, ax = plt.gcf(), plt.gca()\n",
        "fig.set_size_inches(8, 3.5)\n",
        "ax_list = fig.axes\n",
        "ax = ax_list[0]\n",
        "ax.tick_params(labelsize=8)\n",
        "ax.set\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaa949"
      },
      "outputs": [],
      "source": [
        "# Forceplot for a single prediction generated during backtesting\n",
        "# ==============================================================================\n",
        "shap.force_plot(\n",
        "    base_value  = shap_values_single.base_values[iloc_predicted_date],\n",
        "    shap_values = shap_values_single.values[iloc_predicted_date],\n",
        "    features    = predictions.iloc[iloc_predicted_date, 2:],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "1b7d67"
      },
      "source": [
        "TSF=sla,(target tsg,20 goal de aba,offered forecast*2,forecast aht)*100)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5571df"
      },
      "source": [
        "```\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "2732bc"
      },
      "source": [
        "Service Level=  \\(Answered calls in threshold\\)/\\(Agent Offered Calls\\)\n",
        "\n",
        "Ejemplo de una variaci√≥n en la f√≥rmula de SL\n",
        "\n",
        "Service Level \\(2\\)=  \\(Answered calls in threshold\\-abn&lt;10 sec\\)/\\(Agent Offered Calls\\)\n",
        "\n",
        "ASA=\\(Handled Time\\)/\\(Agent handled Calls\\)\n",
        "\n",
        "Abandon=\\(Abandon Calls\\)/\\(Agent Offered Calls\\)\n",
        "\n",
        "Occupancy=1\\-Avail%\n",
        "\n",
        "AHT=\\(BCW\\+TT\\+HT\\+ACW\\)/\\(Agent Handled Calls\\)\n",
        "\n",
        "Calls per hour \\(Agent Handled Calls\\)/\\(Staff Hours\\)\n",
        "\n",
        "Calls per FTE \\(Agent Handled Calls\\)/\\(Total FTE\\)\n",
        "\n",
        "Sells per hour \\(Total Sells\\)/\\(Staff Hours\\)\n",
        "\n",
        "Ratio Calls/Sells \\(Handled Calls\\)/Sells\n",
        "\n",
        "Offered,aht=forecast\n",
        "\n",
        "goals: TSF: Es el porcentaje de llamadas atendidas dentro de un umbral especifico de tiempo\n",
        "\n",
        "TSF Goal \\(75/20\\)\n",
        "\n",
        "75% de las llamadas deben ser atendidas antes de los 20 segundos de espera.\n",
        "\n",
        "TSF = llamadas en Umbral /llamadas ofrecidas\n",
        "\n",
        "AHT: Talk Time \\+ hold time \\+ after call work / numbero de conversasiones de los clientes\n",
        "\n",
        "ABA: Es el porcentaje de llamadas finalizadas por parte del cliente antes de ser atendidas por un CCA\n",
        "\n",
        "ABA Goal: 6%\n",
        "\n",
        "El porcentaje de llamadas abandonadas debe de ser 6%    Llamadas abandonadas / llamadas ofreciadas\n",
        "\n",
        "ASA  es el tiempo promedi oque dura una llamada en la linea de espera antes de ser aatendida por un CCA\n",
        "\n",
        "ASA Goal. 35s donde el tiempo rpomedio de respuesta debe ser hasta 35 segundos\n",
        "\n",
        "AS = Tiempo total de espera / llamadas atendidas\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "fc200d"
      },
      "source": [
        "SCR: Identificaci√≥n de picos: 9:00 am hasta la 1 de la tarde. las horas madrugada son horas con poca informaci√≥n\n",
        "\n",
        "ca: 9 am hasta la 1 de la tarde. En la tarde es de 5 a 7 de la noche.\n",
        "\n",
        "ca: 450 s\n",
        "\n",
        "scr: 520 s\n",
        "\n",
        "aht aumenta los picos cuando offerd aumentado, caudas de aplicativo. problemas de cola horas valle.\n",
        "\n",
        "TSF: %0 ABA 0 segundos ASA.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}