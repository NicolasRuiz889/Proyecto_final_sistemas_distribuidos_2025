{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NicolasRuiz889/Proyecto_final_sistemas_distribuidos_2025/blob/main/copia_de_2025_07_07_file_1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ac1816"
      },
      "source": [
        "\\# MODELO DE PREDICCIÓN PARA OFFERED CALLS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "d6abab"
      },
      "source": [
        "\\#\\#\\# 1. Importación de librerias\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install skforecast"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pqAEm9taw-q",
        "outputId": "61192532-3c51-43db-8385-40cee216d7ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: skforecast in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from skforecast) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.11/dist-packages (from skforecast) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.57 in /usr/local/lib/python3.11/dist-packages (from skforecast) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=1.2 in /usr/local/lib/python3.11/dist-packages (from skforecast) (1.6.1)\n",
            "Requirement already satisfied: optuna>=2.10 in /usr/local/lib/python3.11/dist-packages (from skforecast) (4.4.0)\n",
            "Requirement already satisfied: joblib>=1.1 in /usr/local/lib/python3.11/dist-packages (from skforecast) (1.5.1)\n",
            "Requirement already satisfied: numba>=0.59 in /usr/local/lib/python3.11/dist-packages (from skforecast) (0.60.0)\n",
            "Requirement already satisfied: rich>=13.9 in /usr/local/lib/python3.11/dist-packages (from skforecast) (13.9.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.59->skforecast) (0.43.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna>=2.10->skforecast) (1.16.4)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna>=2.10->skforecast) (6.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna>=2.10->skforecast) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna>=2.10->skforecast) (2.0.41)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna>=2.10->skforecast) (6.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->skforecast) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->skforecast) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->skforecast) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.9->skforecast) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.9->skforecast) (2.19.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2->skforecast) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2->skforecast) (3.6.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna>=2.10->skforecast) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna>=2.10->skforecast) (4.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.9->skforecast) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5->skforecast) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna>=2.10->skforecast) (3.2.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install feature_engine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iu6kGEuLbOpt",
        "outputId": "2cea65b4-e8cb-4fdb-8644-a47e5bd130e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting feature_engine\n",
            "  Downloading feature_engine-1.8.3-py2.py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.11/dist-packages (from feature_engine) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from feature_engine) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from feature_engine) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from feature_engine) (1.15.3)\n",
            "Requirement already satisfied: statsmodels>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from feature_engine) (0.14.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.0->feature_engine) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.0->feature_engine) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.0->feature_engine) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.0->feature_engine) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.0->feature_engine) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.11.1->feature_engine) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.11.1->feature_engine) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->feature_engine) (1.17.0)\n",
            "Downloading feature_engine-1.8.3-py2.py3-none-any.whl (378 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.6/378.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: feature_engine\n",
            "Successfully installed feature_engine-1.8.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install astral"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWu4fYxPbZ4w",
        "outputId": "f199d097-298e-49da-e4fc-3ce97cd00ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting astral\n",
            "  Downloading astral-3.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading astral-3.2-py3-none-any.whl (38 kB)\n",
            "Installing collected packages: astral\n",
            "Successfully installed astral-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3e707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "06739b55-a7b7-464d-f930-409e4e49a87b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[38;5;208mVersion skforecast: 0.16.0\n",
            "\u001b[1m\u001b[38;5;208mVersion scikit-learn: 1.6.1\n",
            "\u001b[1m\u001b[38;5;208mVersion lightgbm: 4.5.0\n",
            "\u001b[1m\u001b[38;5;208mVersion pandas: 2.2.2\n",
            "\u001b[1m\u001b[38;5;208mVersion numpy: 2.0.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap>:1047: ImportWarning:\n",
            "\n",
            "_PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning:\n",
            "\n",
            "_BokehImportHook.find_spec() not found; falling back to find_module()\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#==============================================================================\n",
        "# ⚙️ Configuración inicial\n",
        "# ==============================================================================\n",
        "import warnings\n",
        "warnings.filterwarnings('once')\n",
        "\n",
        "# Colores para impresión\n",
        "color = '\\033[1m\\033[38;5;208m'\n",
        "\n",
        "# ==============================================================================\n",
        "# 🧮 Librerías base\n",
        "# ==============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from io import StringIO\n",
        "import contextlib\n",
        "\n",
        "# ==============================================================================\n",
        "# 📊 Visualización\n",
        "# ==============================================================================\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "plt.rcParams.update({'font.size': 8})\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "import plotly.offline as poff\n",
        "pio.templates.default = \"seaborn\"\n",
        "poff.init_notebook_mode(connected=True)\n",
        "\n",
        "# Statsmodels\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.stattools import acf, pacf\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# skforecast visual tools\n",
        "from skforecast.plot import (\n",
        "    plot_residuals,\n",
        "    calculate_lag_autocorrelation,\n",
        "    set_dark_theme\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# 📦 Librerías de modelos y forecasting\n",
        "# ==============================================================================\n",
        "import sklearn\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.feature_selection import RFECV\n",
        "\n",
        "import lightgbm\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "import shap\n",
        "\n",
        "# skforecast (forecasting con regresores)\n",
        "import skforecast\n",
        "from skforecast.datasets import fetch_dataset\n",
        "from skforecast.feature_selection import select_features\n",
        "from skforecast.preprocessing import RollingFeatures\n",
        "from skforecast.model_selection import (\n",
        "    TimeSeriesFold,\n",
        "    backtesting_forecaster,\n",
        "    backtesting_sarimax,\n",
        "    bayesian_search_forecaster,\n",
        "    grid_search_sarimax\n",
        ")\n",
        "from skforecast.direct import ForecasterDirect\n",
        "from skforecast.recursive import (\n",
        "    ForecasterRecursive,\n",
        "    ForecasterEquivalentDate,\n",
        "    ForecasterSarimax\n",
        ")\n",
        "from skforecast.metrics import calculate_coverage\n",
        "from skforecast.sarimax import Sarimax\n",
        "\n",
        "# ==============================================================================\n",
        "# 🕒 Ingeniería de características temporales\n",
        "# ==============================================================================\n",
        "from feature_engine.datetime import DatetimeFeatures\n",
        "from feature_engine.creation import CyclicalFeatures\n",
        "from feature_engine.timeseries.forecasting import WindowFeatures\n",
        "\n",
        "# Astral para variables solares\n",
        "from astral import LocationInfo\n",
        "from astral.sun import sun\n",
        "\n",
        "# ==============================================================================\n",
        "# 🔎 Mostrar versiones principales\n",
        "# ==============================================================================\n",
        "print(f\"{color}Version skforecast: {skforecast.__version__}\")\n",
        "print(f\"{color}Version scikit-learn: {sklearn.__version__}\")\n",
        "print(f\"{color}Version lightgbm: {lightgbm.__version__}\")\n",
        "print(f\"{color}Version pandas: {pd.__version__}\")\n",
        "print(f\"{color}Version numpy: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amrrXT-NSYdH"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Leer todo el Excel\n",
        "file_path = r'C:\\Users\\nicol\\OneDrive\\Escritorio\\forecast\\data (20).xlsx'\n",
        "df_excel = pd.read_excel(file_path)\n",
        "\n",
        "# 2) Guardar la primera aparición de “Total” en DAY - Year\n",
        "year_total_df = df_excel[df_excel['DAY - Year'] == 'Total'].head(1).copy()\n",
        "\n",
        "# 3) Encontrar el índice de esa fila\n",
        "mask_year_total   = df_excel['DAY - Year'] == 'Total'\n",
        "first_total_idx   = mask_year_total[mask_year_total].index[0]\n",
        "\n",
        "# 4) Cortar todo lo que viene desde esa fila en adelante\n",
        "df_trim = df_excel.iloc[:first_total_idx].copy()\n",
        "\n",
        "# 5) A partir de aquí trabajas solo con df_trim\n",
        "#    Por ejemplo, eliminar cualquier “Total” residual en Month/Day/Hour\n",
        "df_clean = df_trim[\n",
        "    (df_trim['DAY - Month'] != 'Total') &\n",
        "    (df_trim['DAY - Day']   != 'Total') &\n",
        "    (df_trim['Hour']        != 'Total')\n",
        "].copy()\n",
        "\n",
        "# 6) (Opcional) ya no haces dropna si no quieres eliminar NaNs legítimos\n",
        "# 7) Normalizar Hour, construir fecha, etc.\n",
        "\n",
        "# --- Al final tienes ---\n",
        "print(\"Fila de Total en Year guardada:\")\n",
        "print(year_total_df)\n",
        "print(\"\\nData limpia lista para procesar:\")\n",
        "print(df_clean.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oP4al9toSYdH"
      },
      "outputs": [],
      "source": [
        "df_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2omwZ-J8SYdI"
      },
      "outputs": [],
      "source": [
        "# 2) Conteo de nulos por columna\n",
        "null_counts = df_clean.isna().sum().reset_index()\n",
        "null_counts.columns = ['Column', 'Null Count']\n",
        "print(\"Null counts per column:\")\n",
        "print(null_counts.to_string(index=False))\n",
        "\n",
        "# 3) Filas con al menos un nulo\n",
        "nan_rows = df_clean[df_clean.isna().any(axis=1)]\n",
        "print(\"\\nRows containing any nulls:\")\n",
        "print(nan_rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFyjcw7OSYdI"
      },
      "outputs": [],
      "source": [
        "# 1) Normaliza Hour para quedarte solo con \"HH:MM\"\n",
        "df_clean['Hour_str'] = df_clean['Hour'].astype(str).str[:5]\n",
        "\n",
        "# 2) Construye la cadena y parsea\n",
        "fecha_str = (\n",
        "    df_clean['DAY - Year'].astype(int).astype(str)   + '-' +\n",
        "    df_clean['DAY - Month']                          + '-' +\n",
        "    df_clean['DAY - Day'].astype(int).astype(str)    + ' ' +\n",
        "    df_clean['Hour_str']\n",
        ")\n",
        "df_clean['fecha'] = pd.to_datetime(fecha_str,\n",
        "                                   format='%Y-%B-%d %H:%M',\n",
        "                                   errors='raise')  # te avisará si aún hay algo mal\n",
        "\n",
        "# 3) (Opcional) quita la columna auxiliar\n",
        "df_clean.drop(columns=['Hour_str'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sD3q1hEoSYdI"
      },
      "outputs": [],
      "source": [
        "df_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkwCrudMSYdJ"
      },
      "outputs": [],
      "source": [
        "# 7. Guardar el resultado limpio\n",
        "output_csv = r'C:\\Users\\nicol\\OneDrive\\Escritorio\\forecast\\output.csv'\n",
        "df_clean.to_csv(output_csv, index=False)\n",
        "\n",
        "print(\"CSV limpio guardado en:\", output_csv)\n",
        "print(\"\\nFila de total mensual extraída:\")\n",
        "print(month_total_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3_QvcJQSYdJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --- 1. Carga y preparación de df1 ---\n",
        "df1 = pd.read_csv('output.csv', parse_dates=['fecha'], index_col='fecha')\n",
        "\n",
        "df1 = df1.rename(columns={\n",
        "    'Offered':   'OFF Calls',\n",
        "    'Handled':   'Handled Calls',\n",
        "    'AHT':       'Real AHT',\n",
        "    'TSF':       'Real TSF',\n",
        "    'ABA %':     'Real ABA%',\n",
        "    'Calls SL':  'Real ABA',\n",
        "    'ASA':       'Real ASA'\n",
        "})\n",
        "\n",
        "# df1 ya estaba en float, pero nos aseguramos de que TSF y ABA% sean float\n",
        "df1['Real TSF']  = df1['Real TSF'].astype(float)\n",
        "df1['Real ABA%'] = df1['Real ABA%'].astype(float)\n",
        "\n",
        "cols = ['OFF Calls','Handled Calls','Real AHT','Real TSF','Real ABA%','Real ASA','Real ABA']\n",
        "df1 = df1[cols]\n",
        "\n",
        "# --- 2. Carga y preparación de df2 ---\n",
        "df2 = pd.read_csv('data_jam.csv')\n",
        "\n",
        "meses = {\n",
        "    'enero':'January','febrero':'February','marzo':'March','abril':'April',\n",
        "    'mayo':'May','junio':'June','julio':'July','agosto':'August',\n",
        "    'septiembre':'September','octubre':'October','noviembre':'November','diciembre':'December'\n",
        "}\n",
        "\n",
        "df2['mes_ing'] = df2['DATE_S - Mes'].str.lower().map(meses)\n",
        "df2['fecha']   = pd.to_datetime(\n",
        "    df2['DATE_S - Día'].astype(str) + ' ' +\n",
        "    df2['mes_ing'] + ' ' +\n",
        "    df2['DATE_S - Año'].astype(str),\n",
        "    format='%d %B %Y',\n",
        "    errors='coerce'\n",
        ")\n",
        "\n",
        "# Generar índice continuo cada 30 minutos\n",
        "inicio = df2['fecha'].iloc[0]\n",
        "df2['fecha'] = [inicio + pd.Timedelta(minutes=30 * i) for i in range(len(df2))]\n",
        "# 4. CONFIGURACIÓN DE SERIE TEMPORAL\n",
        "df2 = df2.set_index('fecha')\n",
        "df2 = df2.sort_index()\n",
        "df2 = df2.asfreq('30min')\n",
        "\n",
        "# Convertir TSF y ABA% a float\n",
        "df2['Real TSF']  = df2['Real TSF'].str.replace('%','').astype(float) / 100\n",
        "df2['Real ABA%'] = df2['Real ABA%'].str.replace('%','').astype(float) / 100\n",
        "\n",
        "\n",
        "# Asegurar el resto de columnas están numéricas inicialmente\n",
        "df2['OFF Calls']     = df2['OFF Calls'].astype(int)\n",
        "df2['Handled Calls'] = df2['Handled Calls'].astype(int)\n",
        "df2['Real AHT']      = df2['Real AHT'].astype(int)\n",
        "df2['Real ASA']      = df2['Real ASA'].astype(int)\n",
        "df2['Real ABA']      = df2['Real ABA'].astype(int)\n",
        "\n",
        "df2 = df2[cols]\n",
        "\n",
        "# --- 3. Unificación ---\n",
        "cutoff   = df2.index.max() + pd.Timedelta(minutes=30)\n",
        "df1_tail = df1[df1.index >= cutoff]\n",
        "df_final = pd.concat([df2, df1_tail])\n",
        "\n",
        "# --- 4. Downcast de enteros para ahorrar espacio ---\n",
        "int_cols = ['OFF Calls','Handled Calls','Real AHT','Real ASA','Real ABA']\n",
        "df_final[int_cols] = df_final[int_cols].apply(pd.to_numeric, downcast='integer')\n",
        "df_final[['Real TSF','Real ABA%']] = (df_final[['Real TSF','Real ABA%']] * 100).round(1)\n",
        "\n",
        "# --- 5. Guardar y mostrar resultados ---\n",
        "merged_path = 'merged_output.csv'\n",
        "df_final.to_csv(merged_path)\n",
        "print(\"Archivo unificado guardado como:\", merged_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4qVFSwoSYdK"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('merged_output.csv' , parse_dates=['fecha'], index_col='fecha')\n",
        "df.index = pd.to_datetime(df.index)\n",
        "df = df.asfreq('30min')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccf3d8"
      },
      "outputs": [],
      "source": [
        "df.head(80000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeEq3W7ASYdK"
      },
      "outputs": [],
      "source": [
        "df.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh7_sDjGSYdK"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-dmR0UYSYdL"
      },
      "outputs": [],
      "source": [
        "# 2) Conteo de nulos por columna\n",
        "null_counts = df.isna().sum().reset_index()\n",
        "null_counts.columns = ['Column', 'Null Count']\n",
        "print(\"Null counts per column:\")\n",
        "print(null_counts.to_string(index=False))\n",
        "\n",
        "# 3) Filas con al menos un nulo\n",
        "nan_rows = df[df.isna().any(axis=1)]\n",
        "print(\"\\nRows containing any nulls:\")\n",
        "print(nan_rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGQkQUy-SYdL"
      },
      "outputs": [],
      "source": [
        "for col in ['TSF','ABA %','AHT']:\n",
        "    df_clean[col] = df_clean[col].interpolate(\n",
        "        method='time',       # usa la distancia en el índice datetime\n",
        "        limit_direction='both'\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV4t-tMzSYdL"
      },
      "outputs": [],
      "source": [
        "# 2. Filtro a partir del 1-Oct-2024\n",
        "df_filt = df[df.index >= '2024-10-01']\n",
        "\n",
        "# 3. Redondeo TSF y ABA% a 1 decimal\n",
        "df_filt['Real TSF']  = df_filt['Real TSF'].round(1)\n",
        "df_filt['Real ABA%'] = df_filt['Real ABA%'].round(1)\n",
        "\n",
        "# 4. Agrupo y cuento cada par\n",
        "pair_counts = (\n",
        "    df_filt\n",
        "    .groupby(['Real TSF', 'Real ABA%'])\n",
        "    .size()\n",
        "    .reset_index(name='Count')\n",
        "    # Ordeno por Real TSF ascendente (y, de paso, por Real ABA% ascendente)\n",
        "    .sort_values(['Real TSF'])\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# 5. Muestro el resultado\n",
        "print(pair_counts.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15e536"
      },
      "outputs": [],
      "source": [
        "# Verify that a temporary index is complete\n",
        "# ==============================================================================\n",
        "start_date = df.index.min()\n",
        "end_date = df.index.max()\n",
        "complete_date_range = pd.date_range(start=start_date, end=end_date, freq=df.index.freq)\n",
        "is_index_complete = (df.index == complete_date_range).all()\n",
        "print(f\"Index complete: {is_index_complete}\")\n",
        "print(f\"Number of rows with missing values: {df.isnull().any(axis=1).mean()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peIRA5OeSYdM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# matriz booleana de NaNs\n",
        "mask = df.isna().values\n",
        "rows, cols = np.where(mask)\n",
        "\n",
        "for r, c in zip(rows, cols):\n",
        "    fecha = df.index[r]\n",
        "    columna = df.columns[c]\n",
        "    print(f\"NaN en fila índice={r} (fecha={fecha}), columna='{columna}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "989302"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "728d52"
      },
      "outputs": [],
      "source": [
        "print(\"Puntos faltantes:\", df.isna().sum().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d72017"
      },
      "outputs": [],
      "source": [
        "df.head(80000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "694855"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd9379"
      },
      "outputs": [],
      "source": [
        "df = df.loc['2023-01-01 00:00:00':'2025-06-30 23:59:00'].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HILGdQvSYdN"
      },
      "outputs": [],
      "source": [
        "df.index = pd.to_datetime(df.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d89089"
      },
      "outputs": [],
      "source": [
        "df.head(80000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bef011"
      },
      "outputs": [],
      "source": [
        "# ==== División de serie temporal sin solapamientos ====\n",
        "def dividir_serie_temporal(df, col, end_train, end_validation):\n",
        "    \"\"\"\n",
        "    Divide una columna de un DataFrame indexado por fecha en tres subconjuntos:\n",
        "    entrenamiento (hasta end_train), validación (después de end_train hasta end_validation),\n",
        "    y prueba (después de end_validation), sin solapamientos.\n",
        "    \"\"\"\n",
        "    serie = df[col]\n",
        "    # Asegurar tipos datetime\n",
        "    end_train = pd.to_datetime(end_train)\n",
        "    end_validation = pd.to_datetime(end_validation)\n",
        "    # determinar frecuencia\n",
        "    freq = df.index.freq or pd.infer_freq(df.index)\n",
        "    # calcular pasos siguientes\n",
        "    next_train = end_train + freq\n",
        "    next_val = end_validation + freq\n",
        "\n",
        "    data_train = serie.loc[:end_train].copy()\n",
        "    data_val = serie.loc[next_train:end_validation].copy()\n",
        "    data_test = serie.loc[next_val:].copy()\n",
        "\n",
        "    print(f\"📊 {col} - Train      : {data_train.index.min()} — {data_train.index.max()}  (n={len(data_train)})\")\n",
        "    print(f\"📊 {col} - Validation : {data_val.index.min()} — {data_val.index.max()}  (n={len(data_val)})\")\n",
        "    print(f\"📊 {col} - Test       : {data_test.index.min()} — {data_test.index.max()}  (n={len(data_test)})\")\n",
        "\n",
        "    return data_train, data_val, data_test\n",
        "\n",
        "\n",
        "# ==== Gráfica de particiones con líneas verticales y retorno de figura ====\n",
        "def graficar_particiones_temporales(data_train, data_val, data_test,\n",
        "                                    variable, end_train, end_validation):\n",
        "    \"\"\"\n",
        "    Grafica las particiones train/validation/test con líneas que marcan las fronteras.\n",
        "    Devuelve el objeto fig para guardado o modificación posterior.\n",
        "    \"\"\"\n",
        "    set_dark_theme()\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=data_train.index, y=data_train,\n",
        "                             mode='lines', name='Train'))\n",
        "    fig.add_trace(go.Scatter(x=data_val.index, y=data_val,\n",
        "                             mode='lines', name='Validation'))\n",
        "    fig.add_trace(go.Scatter(x=data_test.index, y=data_test,\n",
        "                             mode='lines', name='Test'))\n",
        "    # Líneas de partición\n",
        "    fig.add_vline(x=pd.to_datetime(end_train), line=dict(color='gray', dash='dash'))\n",
        "    fig.add_vline(x=pd.to_datetime(end_validation), line=dict(color='gray', dash='dash'))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=f'Intraday 30-minutes {variable}',\n",
        "        xaxis_title=\"Time\",\n",
        "        yaxis_title=variable,\n",
        "        legend_title=\"Partition:\",\n",
        "        width=800, height=400,\n",
        "        margin=dict(l=20, r=20, t=35, b=20),\n",
        "        legend=dict(orientation=\"h\", yanchor=\"top\", y=1, xanchor=\"left\", x=0.001)\n",
        "    )\n",
        "    fig.show()\n",
        "    return fig\n",
        "\n",
        "\n",
        "# ==== Prueba de estacionariedad con interpretación de p-value ====\n",
        "def probar_estacionariedad(df, columna, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Aplica pruebas ADF y KPSS a la serie y sus primeras dos diferencias,\n",
        "    e interpreta si cada serie es estacionaria según alpha.\n",
        "    \"\"\"\n",
        "    serie = df[columna].dropna()\n",
        "    data_diff_1 = serie.diff().dropna()\n",
        "    data_diff_2 = data_diff_1.diff().dropna()\n",
        "\n",
        "    for data, orden in zip([serie, data_diff_1, data_diff_2], [0,1,2]):\n",
        "        adf = adfuller(data)\n",
        "        kpss_ = kpss(data, nlags=\"auto\")\n",
        "        if orden == 0:\n",
        "            print(f\"\\n📈 Serie original:\")\n",
        "        else:\n",
        "            print(f\"\\n🔁 Diferenciada orden {orden}:\")\n",
        "        print(f\"ADF  → estadístico: {adf[0]:.4f}, p-valor: {adf[1]:.4f} \"\n",
        "              f\"-> {'Estacionaria' if adf[1]<alpha else 'No estacionaria'}\")\n",
        "        print(f\"KPSS → estadístico: {kpss_[0]:.4f}, p-valor: {kpss_[1]:.4f} \"\n",
        "              f\"-> {'No estacionaria' if kpss_[1]<alpha else 'Estacionaria'}\")\n",
        "\n",
        "    # Gráficas\n",
        "    fig, axs = plt.subplots(nrows=3, ncols=1, figsize=(7,5), sharex=True)\n",
        "    serie.plot(ax=axs[0], title='Serie original')\n",
        "    data_diff_1.plot(ax=axs[1], title='Diferenciada orden 1')\n",
        "    data_diff_2.plot(ax=axs[2], title='Diferenciada orden 2')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# ==== Gráfica completa y zoom robusto ====\n",
        "def graficar_con_zoom(df, columna, zoom_range):\n",
        "    \"\"\"\n",
        "    Grafica la serie completa y un zoom en zoom_range usando máscara para fill_between.\n",
        "    \"\"\"\n",
        "    serie = df[columna]\n",
        "    inicio, fin = pd.to_datetime(zoom_range[0]), pd.to_datetime(zoom_range[1])\n",
        "\n",
        "    fig = plt.figure(figsize=(8,4))\n",
        "    grid = GridSpec(nrows=8, ncols=1, hspace=0.6, wspace=0)\n",
        "\n",
        "    main_ax = fig.add_subplot(grid[:3,:])\n",
        "    serie.plot(ax=main_ax, linewidth=0.5, alpha=0.5)\n",
        "    mask = (serie.index >= inicio) & (serie.index <= fin)\n",
        "    main_ax.fill_between(serie.index[mask], serie.min(), serie.max(),\n",
        "                         facecolor='blue', alpha=0.5, zorder=0)\n",
        "    main_ax.set_title(\n",
        "        f\"{columna} - Serie completa: {serie.index.min().date()} a {serie.index.max().date()}\")\n",
        "\n",
        "    zoom_ax = fig.add_subplot(grid[5:,:])\n",
        "    serie.loc[inicio:fin].plot(ax=zoom_ax, linewidth=1)\n",
        "    zoom_ax.set_title(f\"{columna} - Zoom: {inicio} a {fin}\")\n",
        "\n",
        "    plt.subplots_adjust(hspace=1)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ==== Boxplots estacionales con etiquetas legibles ====\n",
        "def graficar_boxplots_estacionalidad(df, columna):\n",
        "    \"\"\"\n",
        "    Boxplots por mes, día de la semana (con etiquetas) y hora, más promedio por día-hora.\n",
        "    \"\"\"\n",
        "    df_aux = df[[columna]].dropna().copy()\n",
        "    df_aux['month'] = df_aux.index.month\n",
        "    df_aux['week_day'] = df_aux.index.dayofweek  # 0=Lun\n",
        "    df_aux['hour_day'] = df_aux.index.hour\n",
        "\n",
        "    # mensual\n",
        "    fig, ax = plt.subplots(figsize=(7,3))\n",
        "    df_aux.boxplot(column=columna, by='month', ax=ax,\n",
        "                   flierprops={'markersize':3, 'alpha':0.3})\n",
        "    df_aux.groupby('month')[columna].median().plot(style='o-', linewidth=0.8, ax=ax)\n",
        "    ax.set_title(f'{columna} - Distribución mensual')\n",
        "    ax.set_xlabel('Mes')\n",
        "    plt.suptitle('')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # semanal\n",
        "    fig, ax = plt.subplots(figsize=(7,3))\n",
        "    df_aux.boxplot(column=columna, by='week_day', ax=ax,\n",
        "                   flierprops={'markersize':3, 'alpha':0.3})\n",
        "    day_labels = [\"Lun\",\"Mar\",\"Mié\",\"Jue\",\"Vie\",\"Sáb\",\"Dom\"]\n",
        "    ax.set_xticklabels(day_labels)\n",
        "    ax.set_title(f'{columna} - Distribución semanal')\n",
        "    ax.set_ylabel(columna)\n",
        "    plt.suptitle('')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # horaria\n",
        "    fig, ax = plt.subplots(figsize=(7,3))\n",
        "    df_aux.boxplot(column=columna, by='hour_day', ax=ax,\n",
        "                   flierprops={'markersize':3, 'alpha':0.3})\n",
        "    ax.set_title(f'{columna} - Distribución horaria')\n",
        "    ax.set_xlabel('Hora del día')\n",
        "    plt.suptitle('')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # promedio día-hora\n",
        "    fig, ax = plt.subplots(figsize=(7,3))\n",
        "    mean_dh = df_aux.groupby(['week_day','hour_day'])[columna].mean()\n",
        "    mean_dh.unstack(level=0).plot(ax=ax)\n",
        "    ax.set_title(f\"{columna} - Promedio por hora y día\")\n",
        "    ax.set_xlabel(\"Hora del día\")\n",
        "    ax.set_ylabel(columna)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ==== Descomposición STL múltiple y retorno de componentes ====\n",
        "def descomponer_stl(df, columna, periodos, rango=None):\n",
        "    \"\"\"\n",
        "    Descompone la serie con STL usando uno o varios periodos.\n",
        "    Retorna un dict de resultados con trend, seasonal y resid para cada periodo.\n",
        "    \"\"\"\n",
        "    serie = df[columna].dropna()\n",
        "    if rango:\n",
        "        inicio, fin = pd.to_datetime(rango[0]), pd.to_datetime(rango[1])\n",
        "        serie = serie.loc[inicio:fin]\n",
        "    if not isinstance(periodos, (list,tuple)):\n",
        "        periodos = [periodos]\n",
        "\n",
        "    resultados = {}\n",
        "    for periodo in periodos:\n",
        "        stl = STL(serie, period=periodo)\n",
        "        res = stl.fit()\n",
        "        resultados[periodo] = res\n",
        "        fig = res.plot()\n",
        "        fig.suptitle(f\"STL period={periodo}\", fontsize=12)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    return resultados\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ==== Test general de estacionariedad ====\n",
        "def test_estacionariedad(df, columna, rango=None, alpha=0.05):\n",
        "    \"\"\"\n",
        "    ADF y KPSS para toda la serie o un rango específico, con interpretación.\n",
        "    \"\"\"\n",
        "    if rango:\n",
        "        inicio, fin = pd.to_datetime(rango[0]), pd.to_datetime(rango[1])\n",
        "        serie = df[columna].loc[inicio:fin].dropna()\n",
        "        print(f\"\\n📈 Estacionariedad de '{columna}' entre {inicio} y {fin}\")\n",
        "    else:\n",
        "        serie = df[columna].dropna()\n",
        "        print(f\"\\n📈 Estacionariedad de toda la serie '{columna}'\")\n",
        "\n",
        "    adf = adfuller(serie)\n",
        "    kpss_ = kpss(serie, nlags=\"auto\")\n",
        "    print(f\"ADF   → Estadístico: {adf[0]:.4f}, p-value: {adf[1]:.4f} \"\n",
        "          f\"-> {'Estacionaria' if adf[1]<alpha else 'No estacionaria'}\")\n",
        "    print(f\"KPSS  → Estadístico: {kpss_[0]:.4f}, p-value: {kpss_[1]:.4f} \"\n",
        "          f\"-> {'No estacionaria' if kpss_[1]<alpha else 'Estacionaria'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5afbe8"
      },
      "outputs": [],
      "source": [
        "# Ejemplo de llamada:\n",
        "train, val, test = dividir_serie_temporal(df, 'OFF Calls', '2024-10-31 23:59:00', '2025-05-30 23:59:00')\n",
        "fig = graficar_particiones_temporales(train, val, test, 'OFF Calls', '2024-10-31 23:59:00', '2025-05-30 23:59:00')\n",
        "probar_estacionariedad(df, 'OFF Calls', alpha=0.05)\n",
        "graficar_con_zoom(df, 'OFF Calls',('2024-05-01 14:00:00', '2024-06-01 14:00:00'))\n",
        "graficar_boxplots_estacionalidad(df, 'OFF Calls')\n",
        "res_dict = descomponer_stl(df, 'OFF Calls', [48, 48*7], rango=('2024-05-01 14:00:00', '2024-06-01 14:00:00'))\n",
        "test_estacionariedad(df, 'OFF Calls', rango=('2024-05-01 14:00:00','2024-06-01 14:00:00'), alpha=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c0087"
      },
      "outputs": [],
      "source": [
        "# Handled Calls\n",
        "train, val, test = dividir_serie_temporal(df, 'Handled Calls', '2024-10-31 23:59:00', '2025-05-31 23:59:00')\n",
        "fig = graficar_particiones_temporales(train, val, test, 'Handled Calls', '2024-10-31 23:59:00', '2025-05-31 23:59:00')\n",
        "probar_estacionariedad(df, 'Handled Calls', alpha=0.05)\n",
        "graficar_con_zoom(df, 'Handled Calls', ('2024-05-01 14:00:00', '2024-06-01 14:00:00'))\n",
        "graficar_boxplots_estacionalidad(df, 'Handled Calls')\n",
        "res_dict = descomponer_stl(df, 'Handled Calls', [48, 48*7], rango=('2024-05-01 14:00:00', '2024-06-01 14:00:00'))\n",
        "test_estacionariedad(df, 'Handled Calls', rango=('2024-05-01 14:00:00', '2024-06-01 14:00:00'), alpha=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bc076"
      },
      "outputs": [],
      "source": [
        "# Real AHT\n",
        "train, val, test = dividir_serie_temporal(df, 'Real AHT', '2024-10-31 23:59:00', '2025-05-31 23:59:00')\n",
        "fig = graficar_particiones_temporales(train, val, test, 'Real AHT', '2024-10-31 23:59:00', '2025-05-31 23:59:00')\n",
        "probar_estacionariedad(df, 'Real AHT', alpha=0.05)\n",
        "graficar_con_zoom(df, 'Real AHT', ('2024-05-01 14:00:00', '2024-06-01 14:00:00'))\n",
        "graficar_boxplots_estacionalidad(df, 'Real AHT')\n",
        "res_dict = descomponer_stl(df, 'Real AHT', [48, 48*7], rango=('2024-05-01 14:00:00', '2024-06-01 14:00:00'))\n",
        "test_estacionariedad(df, 'Real AHT', rango=('2024-05-01 14:00:00', '2024-06-01 14:00:00'), alpha=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71b911"
      },
      "outputs": [],
      "source": [
        "# Real TSF\n",
        "train, val, test = dividir_serie_temporal(df, 'Real TSF', '2024-10-31 23:59:00', '2025-05-31 23:59:00')\n",
        "fig = graficar_particiones_temporales(train, val, test, 'Real TSF', '2024-10-31 23:59:00', '2025-05-31 23:59:00')\n",
        "probar_estacionariedad(df, 'Real TSF', alpha=0.05)\n",
        "graficar_con_zoom(df, 'Real TSF', ('2024-05-01 14:00:00', '2024-06-01 14:00:00'))\n",
        "graficar_boxplots_estacionalidad(df, 'Real TSF')\n",
        "res_dict = descomponer_stl(df, 'Real TSF', [48, 48*7], rango=('2024-05-01 14:00:00', '2024-06-01 14:00:00'))\n",
        "test_estacionariedad(df, 'Real TSF', rango=('2024-05-01 14:00:00', '2024-06-01 14:00:00'), alpha=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4be791"
      },
      "outputs": [],
      "source": [
        "# Real ABA\n",
        "train, val, test = dividir_serie_temporal(df, 'Real ABA', '2024-10-31 23:59:00', '2025-05-31 23:59:00')\n",
        "fig = graficar_particiones_temporales(train, val, test, 'Real ABA', '2024-10-31 23:59:00', '2025-05-31 23:59:00')\n",
        "probar_estacionariedad(df, 'Real ABA', alpha=0.05)\n",
        "graficar_con_zoom(df, 'Real ABA', ('2024-05-01 14:00:00', '2024-06-01 14:00:00'))\n",
        "graficar_boxplots_estacionalidad(df, 'Real ABA')\n",
        "res_dict = descomponer_stl(df, 'Real ABA', [48, 48*7], rango=('2024-05-01 14:00:00', '2024-06-01 14:00:00'))\n",
        "test_estacionariedad(df, 'Real ABA', rango=('2024-05-01 14:00:00', '2024-06-01 14:00:00'), alpha=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dae048"
      },
      "outputs": [],
      "source": [
        "# Real ASA\n",
        "train, val, test = dividir_serie_temporal(df, 'Real ASA', '2024-10-31 23:59:00', '2025-05-31 23:59:00')\n",
        "fig = graficar_particiones_temporales(train, val, test, 'Real ASA', '2024-10-31 23:59:00', '2025-05-31 23:59:00')\n",
        "probar_estacionariedad(df, 'Real ASA', alpha=0.05)\n",
        "graficar_con_zoom(df, 'Real ASA', ('2024-05-01 14:00:00', '2024-06-01 14:00:00'))\n",
        "graficar_boxplots_estacionalidad(df, 'Real ASA')\n",
        "res_dict = descomponer_stl(df, 'Real ASA', [48, 48*7], rango=('2024-05-01 14:00:00', '2024-06-01 14:00:00'))\n",
        "test_estacionariedad(df, 'Real ASA', rango=('2024-05-01 14:00:00', '2024-06-01 14:00:00'), alpha=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "628919"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "8010f5"
      },
      "source": [
        "#### 2. Autocorrelacion y autocorrealcion parcial de targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "096546"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from statsmodels.tsa.stattools import adfuller, kpss, acf, pacf\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "import plotly.graph_objects as go\n",
        "from skforecast.plot import set_dark_theme\n",
        "from matplotlib.gridspec import GridSpec\n",
        "\n",
        "# ==== División de serie temporal sin solapamientos ====\n",
        "# ... (funciones previas mantendrían su lugar)\n",
        "\n",
        "\n",
        "# ==== Autocorrelación y Autocorrelación Parcial ====\n",
        "def plot_autocorrelation(data, columna_objetivo, orden_diff=0, lags=120, figsize=(5, 2)):\n",
        "    \"\"\"\n",
        "    Grafica la función de autocorrelación (ACF) para `columna_objetivo`,\n",
        "    opcionalmente diferenciada `orden_diff` veces.\n",
        "    \"\"\"\n",
        "    serie = data[columna_objetivo].dropna()\n",
        "    if orden_diff > 0:\n",
        "        serie = serie.diff(periods=orden_diff).dropna()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    plot_acf(serie, ax=ax, lags=lags)\n",
        "    ax.set_title(f\"ACF - {columna_objetivo} (diff={orden_diff})\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_partial_autocorrelation(data, columna_objetivo, orden_diff=0, lags=120, figsize=(5, 2)):\n",
        "    \"\"\"\n",
        "    Grafica la función de autocorrelación parcial (PACF) para `columna_objetivo`,\n",
        "    opcionalmente diferenciada `orden_diff` veces.\n",
        "    \"\"\"\n",
        "    serie = data[columna_objetivo].dropna()\n",
        "    if orden_diff > 0:\n",
        "        serie = serie.diff(periods=orden_diff).dropna()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    plot_pacf(serie, ax=ax, lags=lags, method='ywm')\n",
        "    ax.set_title(f\"PACF - {columna_objetivo} (diff={orden_diff})\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "\n",
        "def calcular_lag_autocorrelaciones(\n",
        "    serie: pd.Series,\n",
        "    n_lags: int = 120,\n",
        "    orden_diff: int = 0,\n",
        "    sort_by: str = \"partial_autocorrelation_abs\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calcula ACF y PACF de `serie` con `n_lags`, tras diferenciar `orden_diff` veces.\n",
        "\n",
        "    Retorna DataFrame ordenado según `sort_by`.\n",
        "    \"\"\"\n",
        "    if not isinstance(serie, pd.Series):\n",
        "        raise ValueError(\"La entrada 'serie' debe ser una pd.Series\")\n",
        "\n",
        "    s = serie.dropna()\n",
        "    if orden_diff > 0:\n",
        "        s = s.diff(periods=orden_diff).dropna()\n",
        "\n",
        "    acf_vals = acf(s, nlags=n_lags)\n",
        "    pacf_vals = pacf(s, nlags=n_lags, method='ywm')\n",
        "\n",
        "    df_lags = pd.DataFrame({\n",
        "        \"lag\": range(len(acf_vals)),\n",
        "        \"autocorrelation\": acf_vals,\n",
        "        \"partial_autocorrelation\": pacf_vals\n",
        "    })\n",
        "    df_lags[\"autocorrelation_abs\"] = df_lags[\"autocorrelation\"].abs()\n",
        "    df_lags[\"partial_autocorrelation_abs\"] = df_lags[\"partial_autocorrelation\"].abs()\n",
        "    df_lags = df_lags[df_lags[\"lag\"] > 0]\n",
        "\n",
        "    if sort_by in df_lags.columns:\n",
        "        df_lags = df_lags.sort_values(by=sort_by, ascending=False)\n",
        "\n",
        "    return df_lags.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "369dd8"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'OFF Calls', orden_diff=0, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'OFF Calls', orden_diff=0, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['OFF Calls'], n_lags=120, orden_diff=0, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ca7e8"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'OFF Calls', orden_diff=1, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'OFF Calls', orden_diff=1, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['OFF Calls'], n_lags=120, orden_diff=1, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfa034"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'OFF Calls', orden_diff=2, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'OFF Calls', orden_diff=2, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['OFF Calls'], n_lags=120, orden_diff=2, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "732b0b"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Handled Calls', orden_diff=0, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Handled Calls', orden_diff=0, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Handled Calls'], n_lags=120, orden_diff=0, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67e3ea"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Handled Calls', orden_diff=1, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Handled Calls', orden_diff=1, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Handled Calls'], n_lags=120, orden_diff=1, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8949ac"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Handled Calls', orden_diff=2, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Handled Calls', orden_diff=2, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Handled Calls'], n_lags=120, orden_diff=2, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9651e2"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Real AHT', orden_diff=0, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Real AHT', orden_diff=0, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Real AHT'], n_lags=120, orden_diff=0, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1afa5c"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Real AHT', orden_diff=1, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Real AHT', orden_diff=1, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Real AHT'], n_lags=120, orden_diff=1, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0578b2"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Real AHT', orden_diff=2, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Real AHT', orden_diff=2, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Real AHT'], n_lags=120, orden_diff=2, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07e4d0"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Real TSF', orden_diff=0, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Real TSF', orden_diff=0, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Real TSF'], n_lags=120, orden_diff=0, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0839c"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Real TSF', orden_diff=1, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Real TSF', orden_diff=1, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Real TSF'], n_lags=120, orden_diff=1, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d1c7a"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Real TSF', orden_diff=2, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Real TSF', orden_diff=2, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Real TSF'], n_lags=120, orden_diff=2, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6a7d5"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Real ABA', orden_diff=0, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Real ABA', orden_diff=0, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Real ABA'], n_lags=120, orden_diff=0, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3fd98"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Real ABA', orden_diff=1, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Real ABA', orden_diff=1, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Real ABA'], n_lags=120, orden_diff=1, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f40706"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Real ABA', orden_diff=2, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Real ABA', orden_diff=2, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Real ABA'], n_lags=120, orden_diff=2, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8079f"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Real ASA', orden_diff=0, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Real ASA', orden_diff=0, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Real ASA'], n_lags=120, orden_diff=0, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9b907"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Real ASA', orden_diff=1, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Real ASA', orden_diff=1, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Real ASA'], n_lags=120, orden_diff=1, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e906e"
      },
      "outputs": [],
      "source": [
        "# ==== Ejemplos de uso ====\n",
        "fig_acf = plot_autocorrelation(df, 'Real ASA', orden_diff=2, lags=120)\n",
        "fig_pacf = plot_partial_autocorrelation(df, 'Real ASA', orden_diff=2, lags=120)\n",
        "top_lags = calcular_lag_autocorrelaciones(df['Real ASA'], n_lags=120, orden_diff=2, sort_by=\"partial_autocorrelation_abs\")\n",
        "print(top_lags.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "c42203"
      },
      "source": [
        "### 3. Baseline antes de usar modelos complejos\n",
        "\n",
        "#### Seasonal Naive Forecasting and backtesting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CzsF9TuSYdW"
      },
      "outputs": [],
      "source": [
        "df.index = pd.to_datetime(df.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trAQpY-hSYdX"
      },
      "outputs": [],
      "source": [
        "df.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eabf9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from skforecast.recursive import ForecasterEquivalentDate\n",
        "from skforecast.model_selection import TimeSeriesFold, backtesting_forecaster\n",
        "\n",
        "def baseline_naive_forecast(\n",
        "    df: pd.DataFrame,\n",
        "    column: str,\n",
        "    end_validation: str,\n",
        "    offset: pd.DateOffset = pd.DateOffset(days=1),\n",
        "    steps: int = 48,\n",
        "    metric: str = 'mean_absolute_error'\n",
        "):\n",
        "    \"\"\"\n",
        "    Construye un baseline naive (valor del mismo instante del día anterior),\n",
        "    evalúa su MAE con backtesting y devuelve la métrica y las predicciones.\n",
        "    \"\"\"\n",
        "    # 1. Preparar y ajustar el forecaster\n",
        "    y = df[column].dropna()\n",
        "    y_train = y.loc[: end_validation]\n",
        "    forecaster = ForecasterEquivalentDate(\n",
        "        offset    = offset,\n",
        "        n_offsets = 1\n",
        "    )\n",
        "    forecaster.fit(y = y_train)\n",
        "\n",
        "    # 2. Configurar el CV\n",
        "    cv = TimeSeriesFold(\n",
        "        initial_train_size = len(y_train),\n",
        "        steps              = steps,\n",
        "        refit              = False\n",
        "    )\n",
        "\n",
        "    # 3. Ejecutar backtesting\n",
        "    metric_value, predictions = backtesting_forecaster(\n",
        "        forecaster = forecaster,\n",
        "        y          = y,\n",
        "        cv         = cv,\n",
        "        metric     = metric\n",
        "    )\n",
        "\n",
        "    # 4. Asegurar un valor numérico escalar de la métrica\n",
        "    try:\n",
        "        metric_float = float(metric_value)\n",
        "    except Exception:\n",
        "        # Si es Series o DataFrame, tomamos la media de sus valores\n",
        "        if hasattr(metric_value, 'mean'):\n",
        "            metric_float = metric_value.mean() if not isinstance(metric_value, pd.DataFrame) \\\n",
        "                            else metric_value.values.mean()\n",
        "        else:\n",
        "            # fallback genérico\n",
        "            metric_float = sum(metric_value) / len(metric_value)\n",
        "\n",
        "    print(f\"✅ Baseline naive [{column}] → {metric.upper()} = {metric_float:.2f}\")\n",
        "    return metric_float, predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd289c"
      },
      "outputs": [],
      "source": [
        "targets = ['OFF Calls', 'Handled Calls', 'Real AHT', 'Real ABA', 'Real TSF', 'Real ASA']\n",
        "end_val = '2025-05-31 23:59:00'\n",
        "results = {}\n",
        "\n",
        "for col in targets:\n",
        "    mae, preds = baseline_naive_forecast(\n",
        "        df,\n",
        "        column         = col,\n",
        "        end_validation = end_val,\n",
        "        offset         = pd.DateOffset(days=1),\n",
        "        steps          = 48,\n",
        "        metric         = 'mean_absolute_error'\n",
        "    )\n",
        "    results[col] = mae\n",
        "\n",
        "print(\"\\nMAE summary:\")\n",
        "for col, mae in results.items():\n",
        "    print(f\" - {col:12}: {mae:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "cf2705"
      },
      "source": [
        "## 5. Empezando modelo para OFF Calls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "baa5ac"
      },
      "source": [
        "#### Multi\\-Step Recursive Forecasting with backtesting\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7341a3"
      },
      "outputs": [],
      "source": [
        "data_train, data_val, data_test = dividir_serie_temporal(df, 'OFF Calls', '2024-10-31 23:59:00', '2025-05-31 23:59:00')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5def55"
      },
      "outputs": [],
      "source": [
        "end_train = '2024-10-31 23:59:00'\n",
        "end_validation = '2025-05-31 23:59:00'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60b645"
      },
      "outputs": [],
      "source": [
        "# Create forecaster\n",
        "# ==============================================================================\n",
        "window_features = RollingFeatures(stats=[\"mean\"], window_sizes=48 * 3)\n",
        "forecaster = ForecasterRecursive(\n",
        "                 regressor       = LGBMRegressor(random_state=15926, verbose=-1),\n",
        "                 lags            = 48,\n",
        "                 window_features = window_features\n",
        "             )\n",
        "\n",
        "# Train forecaster\n",
        "# ==============================================================================\n",
        "forecaster.fit(y=df.loc[:end_validation, 'OFF Calls'])\n",
        "forecaster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3050b9"
      },
      "outputs": [],
      "source": [
        "\n",
        "cv = TimeSeriesFold(\n",
        "        steps              = 48,\n",
        "        initial_train_size = len(df.loc[:end_validation]),\n",
        "        refit              = False\n",
        ")\n",
        "# Backtesting\n",
        "# ==============================================================================\n",
        "metric, predictions = backtesting_forecaster(\n",
        "                          forecaster    = forecaster,\n",
        "                          y             = df['OFF Calls'],\n",
        "                          cv            = cv,\n",
        "                          metric        = 'mean_absolute_error',\n",
        "                          verbose       = True, # Set to False to avoid printing\n",
        "                      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec326e"
      },
      "outputs": [],
      "source": [
        "metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9f10a"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b55833"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "def obtener_muestras_semanales_por_anos(\n",
        "    df: pd.DataFrame,\n",
        "    columna_objetivo: str = 'OFF Calls',\n",
        "    anos: list = None,\n",
        "    semanas: int = 4,\n",
        "    seed: int = 42\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    Extrae muestras de longitud `semanas` semanas para la misma fecha inicio\n",
        "    (mes, día, hora, minuto), pero en diferentes años especificados en `anos`,\n",
        "    ajustándose al rango disponible en el DataFrame.\n",
        "\n",
        "    Parámetros:\n",
        "    - df: DataFrame indexado en datetime.\n",
        "    - columna_objetivo: nombre de la columna a muestrear.\n",
        "    - anos: lista de años (int) para generar cada muestra.\n",
        "    - semanas: duración de cada muestra en semanas.\n",
        "    - seed: semilla para reproducibilidad.\n",
        "\n",
        "    Retorna:\n",
        "    - Listado de dicts con 'inicio', 'fin' y 'valores'.\n",
        "    \"\"\"\n",
        "    # Copiar y ordenar\n",
        "    df2 = df.copy()\n",
        "    df2.index = pd.to_datetime(df2.index)\n",
        "    df2 = df2.sort_index()\n",
        "\n",
        "    # Filtrar años disponibles\n",
        "    años_disponibles = sorted(df2.index.year.unique())\n",
        "    if anos is None:\n",
        "        anos = años_disponibles\n",
        "    else:\n",
        "        anos = [y for y in anos if y in años_disponibles]\n",
        "    if not anos:\n",
        "        raise ValueError(\"No hay años disponibles en el DataFrame para las muestras solicitadas.\")\n",
        "\n",
        "    # Inferir frecuencia y duraciones\n",
        "    freq = df2.index.freq or pd.infer_freq(df2.index)\n",
        "    freq_delta = pd.to_timedelta(freq)\n",
        "    delta = pd.Timedelta(weeks=semanas)\n",
        "\n",
        "    # Índice del primer año y rango válido de inicio\n",
        "    primer_ano = anos[0]\n",
        "    idx_first = df2.loc[str(primer_ano)].index\n",
        "    start_bound = idx_first.min()\n",
        "    end_bound = idx_first.max() - delta + freq_delta\n",
        "    valid_starts = idx_first[(idx_first >= start_bound) & (idx_first <= end_bound)]\n",
        "    if valid_starts.empty:\n",
        "        raise ValueError(\"No hay rangos de inicio válidos para el primer año.\")\n",
        "\n",
        "    # Seleccionar un inicio que exista en todos los años\n",
        "    random.seed(seed)\n",
        "    candidate = None\n",
        "    for _ in range(1000):\n",
        "        ts = random.choice(valid_starts)\n",
        "        fin = ts + delta - freq_delta\n",
        "        if all(\n",
        "            (ts.replace(year=y) >= df2.loc[str(y)].index.min()) and\n",
        "            (fin.replace(year=y) <= df2.loc[str(y)].index.max())\n",
        "            for y in anos\n",
        "        ):\n",
        "            candidate = ts\n",
        "            break\n",
        "    if candidate is None:\n",
        "        raise ValueError(\"No se encontró inicio válido tras múltiples intentos.\")\n",
        "\n",
        "    # Generar muestras para cada año\n",
        "    muestras = []\n",
        "    for y in anos:\n",
        "        inicio = candidate.replace(year=y)\n",
        "        fin = inicio + delta - freq_delta\n",
        "        valores = df2.loc[inicio:fin, columna_objetivo].tolist()\n",
        "        muestras.append({'inicio': inicio, 'fin': fin, 'valores': valores})\n",
        "\n",
        "    return muestras\n",
        "\n",
        "\n",
        "def mostrar_muestras(muestras: list) -> None:\n",
        "    \"\"\"\n",
        "    Imprime cada muestra con detalles de fechas y valores.\n",
        "    \"\"\"\n",
        "    for i, m in enumerate(muestras, 1):\n",
        "        print(f\"📦 Muestra {i}\")\n",
        "        print(f\"   ⏱️ Desde: {m['inicio']}  →  Hasta: {m['fin']}\")\n",
        "        print(f\"   🔢 Valores (primeros 10): {m['valores'][:10]}\")\n",
        "        print(f\"   🔢 Valores (todos): {m['valores']}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93c19f"
      },
      "outputs": [],
      "source": [
        "# Ejemplo:\n",
        "muestras = obtener_muestras_semanales_por_anos(\n",
        "    df,\n",
        "    columna_objetivo='OFF Calls',\n",
        "    anos=[2023,2024,2025],\n",
        "    semanas=4,\n",
        "    seed=42\n",
        ")\n",
        "mostrar_muestras(muestras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e8d16"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from feature_engine.datetime import DatetimeFeatures\n",
        "from feature_engine.creation import CyclicalFeatures\n",
        "# (Optional) define holiday list as in original\n",
        "\n",
        "def crear_variables_exogenas_callcenter(df: pd.DataFrame, columna_objetivo: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Crea variables exógenas para predicción de OFF Calls en un call center (frecuencia 30-min).\n",
        "    Incluye:\n",
        "    - Calendario y codificación cíclica\n",
        "    - Feriados en Jamaica (2021–2025)\n",
        "    - Variables semánticas (fin de semana, hora laboral, pico de horas)\n",
        "    - Estadísticas rodantes sobre el objetivo (media, max, std, CV)\n",
        "    - Indicadores de picos y zonas de trabajo (clusters)\n",
        "\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    y = df[columna_objetivo]\n",
        "\n",
        "    # 1. Calendario\n",
        "    calendar = DatetimeFeatures(\n",
        "        variables='index',\n",
        "        features_to_extract=['month', 'week', 'day_of_week', 'hour'],\n",
        "        drop_original=True\n",
        "    ).fit_transform(df)\n",
        "\n",
        "    # 2. Cíclico\n",
        "    cyc = CyclicalFeatures(\n",
        "        variables=['month','week','day_of_week','hour'],\n",
        "        max_values={'month':12,'week':52,'day_of_week':6,'hour':24},\n",
        "        drop_original=False\n",
        "    ).fit_transform(calendar)\n",
        "\n",
        "    # 3. Feriados (simplificado al 2021-2025)\n",
        "    feriados = pd.to_datetime([\n",
        "        '2021-01-01',  # New Year's Day\n",
        "        '2021-02-17',  # Ash Wednesday\n",
        "        '2021-04-02',  # Good Friday\n",
        "        '2021-04-04',  # Easter Day\n",
        "        '2021-04-05',  # Easter Monday\n",
        "        '2021-05-24',  # Labour Day (in lieu)\n",
        "        '2021-08-01',  # Emancipation Day\n",
        "        '2021-08-02',  # Emancipation Day (in lieu)\n",
        "        '2021-08-06',  # Independence Day\n",
        "        '2021-10-18',  # National Heroes' Day\n",
        "        '2021-12-25',  # Christmas Day\n",
        "        '2021-12-26',  # Boxing Day\n",
        "        '2021-12-27',  # Christmas Day (in lieu)\n",
        "        '2022-01-01',  # New Year's Day\n",
        "        '2022-03-02',  # Ash Wednesday\n",
        "        '2022-04-15',  # Good Friday\n",
        "        '2022-04-17',  # Easter Day\n",
        "        '2022-04-18',  # Easter Monday\n",
        "        '2022-05-23',  # Labour Day\n",
        "        '2022-08-01',  # Emancipation Day\n",
        "        '2022-08-06',  # Independence Day\n",
        "        '2022-10-17',  # National Heroes' Day\n",
        "        '2022-12-25',  # Christmas Day\n",
        "        '2022-12-26',  # Christmas Day (in lieu)\n",
        "        '2022-12-27',  # Boxing Day (in lieu)\n",
        "        '2023-01-01',  # New Year's Day\n",
        "        '2023-02-22',  # Ash Wednesday\n",
        "        '2023-04-07',  # Good Friday\n",
        "        '2023-04-09',  # Easter Day\n",
        "        '2023-04-10',  # Easter Monday\n",
        "        '2023-05-23',  # Labour Day\n",
        "        '2023-08-01',  # Emancipation Day\n",
        "        '2023-08-06',  # Independence Day\n",
        "        '2023-08-07',  # Independence Day (in lieu)\n",
        "        '2023-10-16',  # National Heroes' Day\n",
        "        '2023-12-25',  # Christmas Day\n",
        "        '2023-12-26',  # Boxing Day\n",
        "        '2024-01-01',  # New Year's Day\n",
        "        '2024-02-14',  # Ash Wednesday\n",
        "        '2024-03-29',  # Good Friday\n",
        "        '2024-04-01',  # Easter Monday\n",
        "        '2024-05-23',  # Labour Day\n",
        "        '2024-08-01',  # Emancipation Day\n",
        "        '2024-08-06',  # Independence Day\n",
        "        '2024-10-21',  # National Heroes' Day\n",
        "        '2024-12-25',  # Christmas Day\n",
        "        '2024-12-26',  # Boxing Day\n",
        "        '2025-01-01',  # New Year's Day\n",
        "        '2025-03-05',  # Ash Wednesday\n",
        "        '2025-04-18',  # Good Friday\n",
        "        '2025-04-21',  # Easter Monday\n",
        "        '2025-05-23',  # Labour Day\n",
        "        '2025-08-01',  # Emancipation Day\n",
        "        '2025-08-06',  # Independence Day\n",
        "        '2025-10-21',  # National Heroes' Day\n",
        "        '2025-12-25',  # Christmas Day\n",
        "        '2025-12-26',  # Boxing Day\n",
        "        '2026-01-01',  # New Year's Day\n",
        "        '2026-02-18',  # Ash Wednesday\n",
        "        '2026-04-03',  # Good Friday\n",
        "        '2026-04-06',  # Easter Monday\n",
        "        '2026-05-25',  # Labour Day (observed)\n",
        "        '2026-08-01',  # Emancipation Day\n",
        "        '2026-08-06',  # Independence Day\n",
        "        '2026-10-19',  # National Heroes' Day\n",
        "        '2026-12-25',  # Christmas Day\n",
        "        '2026-12-26',  # Boxing Day\n",
        "        '2027-02-10',  # Ash Wednesday\n",
        "        '2027-03-26',  # Good Friday\n",
        "        '2027-03-29',  # Easter Monday\n",
        "        '2027-05-24',  # Labour Day (observed)\n",
        "        '2027-08-02',  # Emancipation Day (observed)\n",
        "        '2027-08-06',  # Independence Day\n",
        "        '2027-10-18',  # National Heroes' Day\n",
        "        '2027-12-25',  # Christmas Day\n",
        "        '2027-12-26',  # Boxing Day\n",
        "        '2027-12-27',  # Boxing Day (substitute)\n",
        "        '2028-01-01',  # New Year's Day\n",
        "        '2028-03-01',  # Ash Wednesday\n",
        "        '2028-04-14',  # Good Friday\n",
        "        '2028-04-17',  # Easter Monday\n",
        "        '2028-05-23',  # Labour Day\n",
        "        '2028-08-01',  # Emancipation Day\n",
        "        '2028-08-06',  # Independence Day\n",
        "        '2028-10-16',  # National Heroes' Day\n",
        "        '2028-12-25',  # Christmas Day\n",
        "        '2028-12-26',  # Boxing Day\n",
        "\n",
        "    ])\n",
        "    holiday = df.index.normalize().isin(feriados).astype(int)\n",
        "    hol = pd.DataFrame({'holiday':holiday}, index=df.index)\n",
        "    hol['prev_holiday'] = hol['holiday'].shift(48).fillna(0).astype(int)\n",
        "    hol['next_holiday'] = hol['holiday'].shift(-48).fillna(0).astype(int)\n",
        "\n",
        "    # 4. Estadísticas rodantes (rolling) SIN generar NaN en la primera fila\n",
        "    windows = {'1D':48, '3D':48*3, '7D':48*7}\n",
        "    roll = pd.DataFrame(index=df.index)\n",
        "    for label, size in windows.items():\n",
        "        roll[f'roll_mean_{label}'] = y.rolling(window=size, min_periods=1).mean()\n",
        "        roll[f'roll_max_{label}']  = y.rolling(window=size, min_periods=1).max()\n",
        "        # ddof=0 para que std con un solo punto sea 0, no NaN\n",
        "        roll[f'roll_std_{label}']  = (\n",
        "            y.rolling(window=size, min_periods=1)\n",
        "             .std(ddof=0)\n",
        "        )\n",
        "        # coeficiente de variación, sin infinitos ni NaN\n",
        "        roll[f'roll_cv_{label}']   = (\n",
        "            roll[f'roll_std_{label}']\n",
        "            .div( roll[f'roll_mean_{label}'].replace(0, np.nan) )\n",
        "            .fillna(0)\n",
        "        )\n",
        "\n",
        "    # 5. Indicadores de picos y zonas\n",
        "    q33, q66, q95 = np.percentile(y.dropna(), [33, 66, 95])\n",
        "    zone = pd.cut(y, bins=[-np.inf, q33, q66, np.inf], labels=[0,1,2]).astype(int)\n",
        "    peak = (y >= q95).astype(int)\n",
        "    peaks = pd.DataFrame({'zone_cluster':zone, 'is_peak_event':peak}, index=df.index)\n",
        "\n",
        "    # 6. Variables semánticas\n",
        "    logic = pd.DataFrame(index=df.index)\n",
        "    logic['is_weekend']      = df.index.dayofweek.isin([5,6]).astype(int)\n",
        "    logic['is_working_hour'] = ((df.index.hour>=7)&(df.index.hour<18)).astype(int)\n",
        "    logic['is_peak_hour']    = ((df.index.hour>=9)&(df.index.hour<16)).astype(int)\n",
        "\n",
        "    # 7. Concatenar\n",
        "    exog = pd.concat([cyc, hol, roll, peaks, logic], axis=1)\n",
        "\n",
        "    # 8. Eliminar todas las columnas que vinieran de df (incluyendo otros targets)\n",
        "    original_cols = df.columns.tolist()\n",
        "    exog = exog.drop(columns=original_cols, errors='ignore')\n",
        "\n",
        "    # Si quieres asegurarte de no generar NaNs imprevistos:\n",
        "    exog = exog.fillna(0)\n",
        "    return exog\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb1c05"
      },
      "outputs": [],
      "source": [
        "# 1. Importa (si está en otro fichero):\n",
        "# from sampling_utils import crear_variables_exogenas_callcenter\n",
        "\n",
        "# 2. Llama a la función pasándole tu DataFrame y la columna objetivo\n",
        "exog = crear_variables_exogenas_callcenter(\n",
        "    df=df,\n",
        "    columna_objetivo='OFF Calls'\n",
        ")\n",
        "\n",
        "# 3. Comprueba el resultado\n",
        "print(f\"Shape de exógenas: {exog.shape}\")\n",
        "display(exog.head(80000))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8ceed"
      },
      "outputs": [],
      "source": [
        "df.head(80000)\n",
        "print(f\"Shape de data: {df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c59be"
      },
      "outputs": [],
      "source": [
        "df.head(80000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6407e"
      },
      "outputs": [],
      "source": [
        "print(exog.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edfc4c"
      },
      "outputs": [],
      "source": [
        "print('OFF Calls' in exog.columns)  # → False esperado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "053e46"
      },
      "source": [
        "Estas sí las conoces por adelantado. Por ejemplo:\n",
        "- Volumen planificado: turnos por agente\n",
        "- Carga anticipada: reservas, sesiones agendadas, campañas\n",
        "- Métricas históricas programadas: si sabes la cantidad de contactos estimados, o el canal más usado\n",
        "- Condiciones repetitivas: si cada lunes a las 9am hay saturación, puedes modelarlo con calendario + lag.\n",
        "⚙️ O usar lags autoregresiv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c30419"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "def crear_interacciones_exogenas(\n",
        "    exog: pd.DataFrame,\n",
        "    columnas_interaccion: list[str],\n",
        "    degree: int = 2\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Añade interacciones de segundo orden (solo términos cruzados)\n",
        "    a un DataFrame de variables exógenas.\n",
        "\n",
        "    Parámetros\n",
        "    ----------\n",
        "    exog : pd.DataFrame\n",
        "        DataFrame con variables exógenas indexado en datetime.\n",
        "    columnas_interaccion : list[str]\n",
        "        Lista de columnas de `exog` sobre las que generar interacciones.\n",
        "    degree : int, opcional\n",
        "        Grado del polinomio; por defecto 2 (interacciones de segundo orden).\n",
        "\n",
        "    Retorna\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame original concatenado con las nuevas características de interacción.\n",
        "    \"\"\"\n",
        "    # Configurar el transformador para que devuelva un DataFrame de pandas\n",
        "    transformer = PolynomialFeatures(\n",
        "        degree=degree,\n",
        "        interaction_only=True,\n",
        "        include_bias=False\n",
        "    )\n",
        "    transformer.set_output(transform=\"pandas\")\n",
        "\n",
        "    # Ajustar y transformar solo las columnas seleccionadas\n",
        "    df_poly = transformer.fit_transform(exog[columnas_interaccion])\n",
        "\n",
        "    # Eliminar las columnas originales si no las queremos duplicadas\n",
        "    df_poly = df_poly.drop(columns=columnas_interaccion, errors='ignore')\n",
        "\n",
        "    # Renombrar las columnas de interacción para identificarlas\n",
        "    df_poly.columns = [\n",
        "        f\"poly__{col.replace(' ', '__')}\" for col in df_poly.columns\n",
        "    ]\n",
        "    df_poly.index = exog.index\n",
        "\n",
        "    # Concatenar con el DataFrame original\n",
        "    exog_expanded = pd.concat([exog, df_poly], axis=1)\n",
        "    return exog_expanded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "929be2"
      },
      "outputs": [],
      "source": [
        "# 1. Define las columnas sobre las que quieres interacciones\n",
        "poly_cols = [\n",
        "    'month_sin','month_cos',\n",
        "    'week_sin','week_cos',\n",
        "    'day_of_week_sin','day_of_week_cos',\n",
        "    'hour_sin','hour_cos',\n",
        "    'holiday','prev_holiday','next_holiday',\n",
        "    'roll_mean_1D','roll_max_1D','roll_std_1D','roll_cv_1D',\n",
        "    'roll_mean_3D','roll_max_3D','roll_std_3D','roll_cv_3D',\n",
        "    'roll_mean_7D','roll_max_7D','roll_std_7D','roll_cv_7D',\n",
        "    'zone_cluster','is_peak_event',\n",
        "    'is_weekend','is_working_hour','is_peak_hour'\n",
        "]\n",
        "\n",
        "# 2. Aplica la función\n",
        "exog_con_interacciones = crear_interacciones_exogenas(exog, poly_cols)\n",
        "\n",
        "# 3. Verifica el resultado\n",
        "print(exog_con_interacciones.shape)\n",
        "print([c for c in exog_con_interacciones.columns if c.startswith('poly__')][:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9249b"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "from lightgbm import LGBMRegressor\n",
        "from skforecast.model_selection import backtesting_forecaster\n",
        "\n",
        "# ✅ 1. Limpiar columnas exógenas (eliminar duplicados y mantener el orden)\n",
        "exog_features = list(OrderedDict.fromkeys(exog_features))\n",
        "\n",
        "# ✅ 2. Verificar que todas existan en exog_con_interacciones\n",
        "exog_features = [col for col in exog_features if col in exog_con_interacciones.columns]\n",
        "\n",
        "# ✅ 3. Construir DataFrame unificado (merge por índice)\n",
        "data = df[['OFF Calls']].merge(\n",
        "    exog_con_interacciones[exog_features],\n",
        "    left_index=True,\n",
        "    right_index=True,\n",
        "    how='inner').astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b62669"
      },
      "outputs": [],
      "source": [
        "# ✅ 4. Definir fechas de corte (ajusta si usas variables distintas)\n",
        "end_train = '2024-04-30 23:59:00'\n",
        "end_validation = '2025-03-31 23:59:00'\n",
        "\n",
        "# ✅ 5. Dividir en train / val / test\n",
        "data_train = data.loc[:end_train, :].copy()\n",
        "data_val   = data.loc[end_train:end_validation, :].copy()\n",
        "data_test  = data.loc[end_validation:, :].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00bef4"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c5951"
      },
      "outputs": [],
      "source": [
        "# Redefinir el forecaster si no se ha hecho con las nuevas series\n",
        "window_features = RollingFeatures(stats=[\"mean\"], window_sizes=48*3)\n",
        "\n",
        "forecaster = ForecasterRecursive(\n",
        "                 regressor       = LGBMRegressor(random_state=15926, verbose=-1),\n",
        "                 lags            = 48,\n",
        "                 window_features = window_features\n",
        "             )\n",
        "\n",
        "# Entrenamiento usando hasta end_validation (train + val)\n",
        "forecaster.fit(\n",
        "    y    = data.loc[:end_validation, 'OFF Calls'],\n",
        "    exog = data.loc[:end_validation, exog_features]\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25918c"
      },
      "outputs": [],
      "source": [
        "# Configurar folds temporales\n",
        "cv = TimeSeriesFold(\n",
        "    steps              = 48,  # horizonte de predicción\n",
        "    initial_train_size = len(data.loc[:end_validation]),\n",
        "    refit              = False\n",
        ")\n",
        "\n",
        "# Ejecutar backtesting\n",
        "metric, predictions = backtesting_forecaster(\n",
        "    forecaster = forecaster,\n",
        "    y          = data['OFF Calls'],\n",
        "    exog       = data[exog_features],\n",
        "    cv         = cv,\n",
        "    metric     = 'mean_absolute_error',\n",
        "    verbose    = True\n",
        ")\n",
        "\n",
        "# Visualizar resultados\n",
        "display(metric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e05b6b"
      },
      "outputs": [],
      "source": [
        "predictions.head(80000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14b9cb"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objs as go\n",
        "\n",
        "# Asegúrate de que las predicciones estén alineadas con el índice\n",
        "fig = go.Figure()\n",
        "\n",
        "# Trazar valores reales\n",
        "fig.add_trace(go.Scatter(\n",
        "    x = data_test.index,\n",
        "    y = data_test['OFF Calls'],\n",
        "    mode = 'lines',\n",
        "    name = 'Valor real'\n",
        "))\n",
        "\n",
        "# Trazar predicciones (el índice de `predictions` ya está alineado)\n",
        "fig.add_trace(go.Scatter(\n",
        "    x = predictions.index,\n",
        "    y = predictions['pred'],\n",
        "    mode = 'lines',\n",
        "    name = 'Predicción',\n",
        "    line = dict(color='firebrick')\n",
        "))\n",
        "\n",
        "# Configurar layout\n",
        "fig.update_layout(\n",
        "    title = '📊 Predicción vs Real — OFF Calls',\n",
        "    xaxis_title = 'Fecha y hora',\n",
        "    yaxis_title = 'Número de llamadas',\n",
        "    width = 900,\n",
        "    height = 450,\n",
        "    legend = dict(orientation=\"h\", y=1.02, x=0),\n",
        "    margin = dict(l=40, r=40, t=40, b=40)\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f4bc5"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters search\n",
        "# ==============================================================================\n",
        "forecaster = ForecasterRecursive(\n",
        "                 regressor       = LGBMRegressor(random_state=15926, verbose=-1),\n",
        "                 lags            = 48,  # This value will be replaced in the grid search\n",
        "                 window_features = window_features\n",
        "             )\n",
        "\n",
        "# Lags used as predictors\n",
        "lags_grid = [48, (1, 2, 3, 4, 49, 48, 5, 50, 34)]\n",
        "\n",
        "# Regressor hyperparameters search space\n",
        "def search_space(trial):\n",
        "    search_space  = {\n",
        "        'n_estimators' : trial.suggest_int('n_estimators', 300, 1000, step=100),\n",
        "        'max_depth'    : trial.suggest_int('max_depth', 3, 10),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5),\n",
        "        'reg_alpha'    : trial.suggest_float('reg_alpha', 0, 1),\n",
        "        'reg_lambda'   : trial.suggest_float('reg_lambda', 0, 1),\n",
        "        'lags'         : trial.suggest_categorical('lags', lags_grid)\n",
        "    }\n",
        "    return search_space\n",
        "\n",
        "# Folds training and validation\n",
        "cv_search = TimeSeriesFold(\n",
        "                steps              = 48,\n",
        "                initial_train_size = len(data[:end_train]),\n",
        "                refit              = False,\n",
        "            )\n",
        "\n",
        "results_search, frozen_trial = bayesian_search_forecaster(\n",
        "                                   forecaster   = forecaster,\n",
        "                                   y            = data.loc[:end_validation, 'OFF Calls'],\n",
        "                                   exog         = data.loc[:end_validation, exog_features],\n",
        "                                   cv           = cv_search,\n",
        "                                   metric       = 'mean_absolute_error',\n",
        "                                   search_space = search_space,\n",
        "                                   n_trials     = 10,  # Increase for more exhaustive search\n",
        "                                   return_best  = True\n",
        "                               )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84166e"
      },
      "outputs": [],
      "source": [
        "# Search results\n",
        "# ==============================================================================\n",
        "best_params = results_search.at[0, 'params']\n",
        "best_params = best_params | {'random_state': 15926, 'verbose': -1}\n",
        "best_lags = results_search.at[0, 'lags']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bf9b7"
      },
      "outputs": [],
      "source": [
        "results_search.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c6978"
      },
      "outputs": [],
      "source": [
        "# Best model\n",
        "# ==============================================================================\n",
        "forecaster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44dd1e"
      },
      "outputs": [],
      "source": [
        "# Backtest final model on test data\n",
        "# ==============================================================================\n",
        "metric, predictions = backtesting_forecaster(\n",
        "                          forecaster = forecaster,\n",
        "                          y          = data['OFF Calls'],\n",
        "                          exog       = data[exog_features],\n",
        "                          cv         = cv,\n",
        "                          metric     = 'mean_absolute_error'\n",
        "                      )\n",
        "display(metric)\n",
        "predictions.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc155c"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objs as go\n",
        "\n",
        "# Asegúrate de que las predicciones estén alineadas con el índice\n",
        "fig = go.Figure()\n",
        "\n",
        "# Trazar valores reales\n",
        "fig.add_trace(go.Scatter(\n",
        "    x = data_test.index,\n",
        "    y = data_test['OFF Calls'],\n",
        "    mode = 'lines',\n",
        "    name = 'Valor real'\n",
        "))\n",
        "\n",
        "# Trazar predicciones (el índice de `predictions` ya está alineado)\n",
        "fig.add_trace(go.Scatter(\n",
        "    x = predictions.index,\n",
        "    y = predictions['pred'],\n",
        "    mode = 'lines',\n",
        "    name = 'Predicción',\n",
        "    line = dict(color='firebrick')\n",
        "))\n",
        "\n",
        "# Configurar layout\n",
        "fig.update_layout(\n",
        "    title = '📊 Predicción vs Real — OFF Calls',\n",
        "    xaxis_title = 'Fecha y hora',\n",
        "    yaxis_title = 'Número de llamadas',\n",
        "    width = 900,\n",
        "    height = 450,\n",
        "    legend = dict(orientation=\"h\", y=1.02, x=0),\n",
        "    margin = dict(l=40, r=40, t=40, b=40)\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "c42750"
      },
      "source": [
        "### Selección de caracteristicas\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10840b"
      },
      "outputs": [],
      "source": [
        "# Create forecaster\n",
        "# ==============================================================================\n",
        "regressor = LGBMRegressor(\n",
        "                n_estimators = 100,\n",
        "                max_depth    = 4,\n",
        "                random_state = 15926,\n",
        "                verbose      = -1\n",
        "            )\n",
        "\n",
        "forecaster = ForecasterRecursive(\n",
        "                 regressor       = regressor,\n",
        "                 lags            = best_lags,\n",
        "                 window_features = window_features\n",
        "             )\n",
        "\n",
        "# Recursive feature elimination with cross-validation\n",
        "# ==============================================================================\n",
        "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names.*\")\n",
        "selector = RFECV(\n",
        "    estimator = regressor,\n",
        "    step      = 1,\n",
        "    cv        = 3,\n",
        ")\n",
        "lags_select, window_features_select, exog_select = select_features(\n",
        "    forecaster      = forecaster,\n",
        "    selector        = selector,\n",
        "    y               = data_train['OFF Calls'],\n",
        "    exog            = data_train[exog_features],\n",
        "    select_only     = None,\n",
        "    force_inclusion = None,\n",
        "    subsample       = 0.5,  # Subsample to speed up the process\n",
        "    random_state    = 123,\n",
        "    verbose         = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1ff9e"
      },
      "outputs": [],
      "source": [
        "# Create a forecaster with the selected features\n",
        "# ==============================================================================\n",
        "forecaster = ForecasterRecursive(\n",
        "                regressor       = LGBMRegressor(**best_params),\n",
        "                lags            = lags_select,\n",
        "                window_features = window_features\n",
        "             )\n",
        "# Backtesting model with exogenous variables on test data\n",
        "# ==============================================================================\n",
        "metric, predictions = backtesting_forecaster(\n",
        "                            forecaster = forecaster,\n",
        "                            y          = data['OFF Calls'],\n",
        "                            exog       = data[exog_select],\n",
        "                            cv         = cv,\n",
        "                            metric     = 'mean_absolute_error'\n",
        "                      )\n",
        "metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "aa05c8"
      },
      "source": [
        "### Pronostico Probabilistico: Prediccion de los intervalos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b9c92"
      },
      "outputs": [],
      "source": [
        "# Create and train forecaster\n",
        "# ==============================================================================\n",
        "forecaster = ForecasterRecursive(\n",
        "                 regressor       = LGBMRegressor(**best_params),\n",
        "                 lags            = lags_select,\n",
        "                 window_features = window_features,\n",
        "                 binner_kwargs   = {\"n_bins\": 5}\n",
        "             )\n",
        "forecaster.fit(\n",
        "    y    = data.loc[:end_train, 'OFF Calls'],\n",
        "    exog = data.loc[:end_train, exog_select],\n",
        "    store_in_sample_residuals = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63ab04"
      },
      "outputs": [],
      "source": [
        "# Predict intervals\n",
        "# ==============================================================================\n",
        "# Since the model has been trained with exogenous variables, they must be provided\n",
        "# for the prediction.\n",
        "predictions = forecaster.predict_interval(\n",
        "                  exog     = data.loc[end_train:, exog_select],\n",
        "                  steps    = 48,\n",
        "                  interval = [5, 95],\n",
        "                  method  = 'conformal',\n",
        "              )\n",
        "predictions.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1e4f0"
      },
      "outputs": [],
      "source": [
        "# Backtesting on validation data to obtain out-sample residuals\n",
        "# ==============================================================================\n",
        "cv = TimeSeriesFold(\n",
        "        steps              = 48,\n",
        "        initial_train_size = len(data.loc[:end_train]),\n",
        "        refit              = False,\n",
        ")\n",
        "_, predictions_val = backtesting_forecaster(\n",
        "                         forecaster = forecaster,\n",
        "                         y          = data.loc[:end_validation, 'OFF Calls'],\n",
        "                         exog       = data.loc[:end_validation, exog_select],\n",
        "                         cv         = cv,\n",
        "                         metric     = 'mean_absolute_error'\n",
        "                     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daa8e6"
      },
      "outputs": [],
      "source": [
        "# Out-sample residuals distribution\n",
        "# ==============================================================================\n",
        "residuals = data.loc[predictions_val.index, 'OFF Calls'] - predictions_val['pred']\n",
        "print(pd.Series(np.where(residuals < 0, 'negative', 'positive')).value_counts())\n",
        "plt.rcParams.update({'font.size': 8})\n",
        "_ = plot_residuals(residuals=residuals, figsize=(7, 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efc254"
      },
      "outputs": [],
      "source": [
        "# Store out-sample residuals in the forecaster\n",
        "# ==============================================================================\n",
        "forecaster.set_out_sample_residuals(\n",
        "    y_true = data.loc[predictions_val.index, 'OFF Calls'],\n",
        "    y_pred = predictions_val['pred']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5241d"
      },
      "outputs": [],
      "source": [
        "# Backtesting with prediction intervals in test data using out-sample residuals\n",
        "# ==============================================================================\n",
        "cv = TimeSeriesFold(\n",
        "        steps              = 48,\n",
        "        initial_train_size = len(data.loc[:end_validation]),\n",
        "        refit              = False,\n",
        ")\n",
        "metric, predictions = backtesting_forecaster(\n",
        "                            forecaster              = forecaster,\n",
        "                            y                       = data['OFF Calls'],\n",
        "                            exog                    = data[exog_select],\n",
        "                            cv                      = cv,\n",
        "                            metric                  = 'mean_absolute_error',\n",
        "                            interval                = [5, 95],\n",
        "                            interval_method         = 'conformal',\n",
        "                            use_in_sample_residuals = False,  # out-sample residuals\n",
        "                            use_binned_residuals    = True,   # Intervals conditioned to the predicted values\n",
        "                       )\n",
        "predictions.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "734253"
      },
      "outputs": [],
      "source": [
        "# Plot prediction intervals vs real value\n",
        "# ==============================================================================\n",
        "fig = go.Figure([\n",
        "    go.Scatter(\n",
        "        name='Prediction', x=predictions.index, y=predictions['pred'], mode='lines',\n",
        "    ),\n",
        "    go.Scatter(\n",
        "        name='Real value', x=data_test.index, y=data_test['OFF Calls'], mode='lines',\n",
        "    ),\n",
        "    go.Scatter(\n",
        "        name='Upper Bound', x=predictions.index, y=predictions['upper_bound'],\n",
        "        mode='lines', marker=dict(color=\"#444\"), line=dict(width=0), showlegend=False\n",
        "    ),\n",
        "    go.Scatter(\n",
        "        name='Lower Bound', x=predictions.index, y=predictions['lower_bound'],\n",
        "        marker=dict(color=\"#444\"), line=dict(width=0), mode='lines',\n",
        "        fillcolor='rgba(68, 68, 68, 0.3)', fill='tonexty', showlegend=False\n",
        "    )\n",
        "])\n",
        "fig.update_layout(\n",
        "    title=\"Real value vs predicted in test data\",\n",
        "    xaxis_title=\"Date time\",\n",
        "    yaxis_title=\"OFF Calls\",\n",
        "    width=800,\n",
        "    height=400,\n",
        "    margin=dict(l=20, r=20, t=35, b=20),\n",
        "    hovermode=\"x\",\n",
        "    legend=dict(orientation=\"h\", yanchor=\"top\", y=1.1, xanchor=\"left\", x=0.001)\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40b60e"
      },
      "outputs": [],
      "source": [
        "# Predicted interval coverage (on test data)\n",
        "# ==============================================================================\n",
        "coverage = calculate_coverage(\n",
        "              y_true       = data.loc[end_validation:, \"OFF Calls\"],\n",
        "              lower_bound  = predictions[\"lower_bound\"],\n",
        "              upper_bound  = predictions[\"upper_bound\"]\n",
        "           )\n",
        "area = (predictions['upper_bound'] - predictions['lower_bound']).sum()\n",
        "print(f\"Total area of the interval: {round(area, 2)}\")\n",
        "print(f\"Predicted interval coverage: {round(100 * coverage, 2)} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "c42cf3"
      },
      "source": [
        "#### Explicabilidad e interpretabilidad del modelo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76fcad"
      },
      "outputs": [],
      "source": [
        "# Create and train forecaster\n",
        "# ==============================================================================\n",
        "forecaster = ForecasterRecursive(\n",
        "                 regressor       = LGBMRegressor(**best_params),\n",
        "                 lags            = lags_select,\n",
        "                 window_features = window_features\n",
        "             )\n",
        "forecaster.fit(\n",
        "    y    = data.loc[:end_validation, 'OFF Calls'],\n",
        "    exog = data.loc[:end_validation, exog_select]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "363473"
      },
      "outputs": [],
      "source": [
        "# Model-specific feature importances\n",
        "# ==============================================================================\n",
        "feature_importances = forecaster.get_feature_importances()\n",
        "feature_importances.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5ae1ff"
      },
      "source": [
        "#### Shap Values\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa203f"
      },
      "outputs": [],
      "source": [
        "# Training matrices used by the forecaster to fit the internal regressor\n",
        "# ==============================================================================\n",
        "X_train, y_train = forecaster.create_train_X_y(\n",
        "                       y    = data_train['OFF Calls'],\n",
        "                       exog = data_train[exog_select]\n",
        "                   )\n",
        "display(X_train.head(3))\n",
        "display(y_train.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "280f07"
      },
      "outputs": [],
      "source": [
        "# Create SHAP explainer (for three base models)\n",
        "# ==============================================================================\n",
        "shap.initjs()\n",
        "explainer = shap.TreeExplainer(forecaster.regressor)\n",
        "\n",
        "# Sample 50% of the data to speed up the calculation\n",
        "rng = np.random.default_rng(seed=785412)\n",
        "sample = rng.choice(X_train.index, size=int(len(X_train)*0.5), replace=False)\n",
        "X_train_sample = X_train.loc[sample, :]\n",
        "shap_values = explainer.shap_values(X_train_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad1767"
      },
      "outputs": [],
      "source": [
        "# Shap summary plot (top 10)\n",
        "# ==============================================================================\n",
        "shap.summary_plot(shap_values, X_train_sample, max_display=10, show=False)\n",
        "fig, ax = plt.gcf(), plt.gca()\n",
        "ax.set_title(\"SHAP Summary plot\")\n",
        "ax.tick_params(labelsize=8)\n",
        "fig.set_size_inches(6, 4.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04d9df"
      },
      "outputs": [],
      "source": [
        "# Backtesting returning the predictors\n",
        "# ==============================================================================\n",
        "cv = TimeSeriesFold(\n",
        "        steps              = 48,\n",
        "        initial_train_size = len(data.loc[:end_validation]),\n",
        ")\n",
        "_, predictions = backtesting_forecaster(\n",
        "                        forecaster        = forecaster,\n",
        "                        y                 = data['OFF Calls'],\n",
        "                        exog              = data[exog_select],\n",
        "                        cv                = cv,\n",
        "                        metric            = 'mean_absolute_error',\n",
        "                        return_predictors = True,\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dcc41"
      },
      "outputs": [],
      "source": [
        "predictions.head(8000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbd990"
      },
      "outputs": [],
      "source": [
        "# Waterfall for a single prediction generated during backtesting\n",
        "# ==============================================================================\n",
        "predictions = predictions.astype(data[exog_select].dtypes) # Ensure that the types are the same\n",
        "iloc_predicted_date = predictions.index.get_loc('2025-04-01 21:30:00')\n",
        "shap_values_single = explainer(predictions.iloc[:, 2:])\n",
        "shap.plots.waterfall(shap_values_single[iloc_predicted_date], show=False)\n",
        "fig, ax = plt.gcf(), plt.gca()\n",
        "fig.set_size_inches(8, 3.5)\n",
        "ax_list = fig.axes\n",
        "ax = ax_list[0]\n",
        "ax.tick_params(labelsize=8)\n",
        "ax.set\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaa949"
      },
      "outputs": [],
      "source": [
        "# Forceplot for a single prediction generated during backtesting\n",
        "# ==============================================================================\n",
        "shap.force_plot(\n",
        "    base_value  = shap_values_single.base_values[iloc_predicted_date],\n",
        "    shap_values = shap_values_single.values[iloc_predicted_date],\n",
        "    features    = predictions.iloc[iloc_predicted_date, 2:],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "1b7d67"
      },
      "source": [
        "TSF=sla,(target tsg,20 goal de aba,offered forecast*2,forecast aht)*100)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5571df"
      },
      "source": [
        "```\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "2732bc"
      },
      "source": [
        "Service Level=  \\(Answered calls in threshold\\)/\\(Agent Offered Calls\\)\n",
        "\n",
        "Ejemplo de una variación en la fórmula de SL\n",
        "\n",
        "Service Level \\(2\\)=  \\(Answered calls in threshold\\-abn&lt;10 sec\\)/\\(Agent Offered Calls\\)\n",
        "\n",
        "ASA=\\(Handled Time\\)/\\(Agent handled Calls\\)\n",
        "\n",
        "Abandon=\\(Abandon Calls\\)/\\(Agent Offered Calls\\)\n",
        "\n",
        "Occupancy=1\\-Avail%\n",
        "\n",
        "AHT=\\(BCW\\+TT\\+HT\\+ACW\\)/\\(Agent Handled Calls\\)\n",
        "\n",
        "Calls per hour \\(Agent Handled Calls\\)/\\(Staff Hours\\)\n",
        "\n",
        "Calls per FTE \\(Agent Handled Calls\\)/\\(Total FTE\\)\n",
        "\n",
        "Sells per hour \\(Total Sells\\)/\\(Staff Hours\\)\n",
        "\n",
        "Ratio Calls/Sells \\(Handled Calls\\)/Sells\n",
        "\n",
        "Offered,aht=forecast\n",
        "\n",
        "goals: TSF: Es el porcentaje de llamadas atendidas dentro de un umbral especifico de tiempo\n",
        "\n",
        "TSF Goal \\(75/20\\)\n",
        "\n",
        "75% de las llamadas deben ser atendidas antes de los 20 segundos de espera.\n",
        "\n",
        "TSF = llamadas en Umbral /llamadas ofrecidas\n",
        "\n",
        "AHT: Talk Time \\+ hold time \\+ after call work / numbero de conversasiones de los clientes\n",
        "\n",
        "ABA: Es el porcentaje de llamadas finalizadas por parte del cliente antes de ser atendidas por un CCA\n",
        "\n",
        "ABA Goal: 6%\n",
        "\n",
        "El porcentaje de llamadas abandonadas debe de ser 6%    Llamadas abandonadas / llamadas ofreciadas\n",
        "\n",
        "ASA  es el tiempo promedi oque dura una llamada en la linea de espera antes de ser aatendida por un CCA\n",
        "\n",
        "ASA Goal. 35s donde el tiempo rpomedio de respuesta debe ser hasta 35 segundos\n",
        "\n",
        "AS = Tiempo total de espera / llamadas atendidas\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "fc200d"
      },
      "source": [
        "SCR: Identificación de picos: 9:00 am hasta la 1 de la tarde. las horas madrugada son horas con poca información\n",
        "\n",
        "ca: 9 am hasta la 1 de la tarde. En la tarde es de 5 a 7 de la noche.\n",
        "\n",
        "ca: 450 s\n",
        "\n",
        "scr: 520 s\n",
        "\n",
        "aht aumenta los picos cuando offerd aumentado, caudas de aplicativo. problemas de cola horas valle.\n",
        "\n",
        "TSF: %0 ABA 0 segundos ASA.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}